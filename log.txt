2022-04-25 16:07:24,674 maskrcnn_benchmark INFO: Using 2 GPUs
2022-04-25 16:07:24,675 maskrcnn_benchmark INFO: Namespace(config_file='configs/e2e_relation_X_101_32_8_FPN_1x_trans_sgdet.yaml', distributed=True, local_rank='0', opts=['SOLVER.IMS_PER_BATCH', '16', 'TEST.IMS_PER_BATCH', '2', 'DTYPE', 'float32', 'SOLVER.MAX_ITER', '18000', 'SOLVER.STEPS', '(10000,16000)', 'SOLVER.WARMUP_ITERS', '500', 'GLOVE_DIR', 'glove', 'MODEL.PRETRAINED_DETECTOR_CKPT', 'vg_faster_det.pth', 'OUTPUT_DIR', 'dual', 'SOLVER.PRE_VAL', 'False', 'MODEL.ROI_RELATION_HEAD.PREDICTOR', 'DualTransPredictor', 'MODEL.ROI_RELATION_HEAD.DUAL_TRANS.USE_GRAPH_ENCODE', 'True', 'MODEL.ROI_RELATION_HEAD.DUAL_TRANS.GRAPH_ENCODE_STRATEGY', 'trans', 'DATALOADER.NUM_WORKERS', '0'], skip_test=False)
2022-04-25 16:07:24,675 maskrcnn_benchmark INFO: Collecting env info (might take some time)
2022-04-25 16:07:33,132 maskrcnn_benchmark INFO: 
PyTorch version: 1.4.0+cu100
Is debug build: No
CUDA used to build PyTorch: 10.0

OS: Ubuntu 16.04.7 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
CMake version: Could not collect

Python version: 3.8
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration: 
GPU 0: TITAN Xp
GPU 1: TITAN Xp
GPU 2: TITAN Xp
GPU 3: TITAN Xp

Nvidia driver version: 410.93
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip3] numpy==1.21.4
[pip3] torch==1.4.0+cu100
[pip3] torchvision==0.5.0+cu100
[conda] blas                      1.0                         mkl  
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py39h7f8727e_0  
[conda] mkl_fft                   1.3.1            py39hd3c417c_0  
[conda] mkl_random                1.2.2            py39h51133e4_0
        Pillow (8.4.0)
2022-04-25 16:07:33,132 maskrcnn_benchmark INFO: Loaded configuration file configs/e2e_relation_X_101_32_8_FPN_1x_trans_sgdet.yaml
2022-04-25 16:07:33,133 maskrcnn_benchmark INFO: 
GLOVE_DIR: $GLOVE_DIR
CONCEPTNET_DIR: /public/data1/users/chenchao278/data/conceptNet/
INPUT:
  MIN_SIZE_TRAIN: (600,)
  MAX_SIZE_TRAIN: 1000
  MIN_SIZE_TEST: 600
  MAX_SIZE_TEST: 1000
MODEL:
  META_ARCHITECTURE: "GeneralizedRCNN"
  WEIGHT: "catalog://ImageNetPretrained/FAIR/20171220/X-101-32x8d"
  BACKBONE:
    CONV_BODY: "R-101-FPN" # VGG-16
  RESNETS:
    BACKBONE_OUT_CHANNELS: 256
    STRIDE_IN_1X1: False
    NUM_GROUPS: 32
    WIDTH_PER_GROUP: 8
  RELATION_ON: True
  ATTRIBUTE_ON: False
  FLIP_AUG: False            # if there is any left-right relation, FLIP AUG should be false
  RPN:
    USE_FPN: True
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)   # from neural-motifs
    PRE_NMS_TOP_N_TRAIN: 6000
    PRE_NMS_TOP_N_TEST: 6000
    POST_NMS_TOP_N_TRAIN: 1000
    POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_PER_BATCH: False
    RPN_MID_CHANNEL: 256
  ROI_HEADS:
    USE_FPN: True
    POSITIVE_FRACTION: 0.5
    BG_IOU_THRESHOLD: 0.3
    BATCH_SIZE_PER_IMAGE: 256
    DETECTIONS_PER_IMG: 80
    NMS_FILTER_DUPLICATES: True
  ROI_BOX_HEAD:
    POOLER_RESOLUTION: 7
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    POOLER_SAMPLING_RATIO: 2
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    NUM_CLASSES: 151                # 151 for VG, 1201 for GQA
    MLP_HEAD_DIM: 4096
  ROI_ATTRIBUTE_HEAD:
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    USE_BINARY_LOSS: True           # choose binary, because cross_entropy loss deteriorate the box head, even with 0.1 weight
    POS_WEIGHT: 50.0
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    NUM_ATTRIBUTES: 201             # 201 for VG, 501 for GQA
    MAX_ATTRIBUTES: 10             
    ATTRIBUTE_BGFG_SAMPLE: True    
    ATTRIBUTE_BGFG_RATIO: 3        
  ROI_RELATION_HEAD:
    USE_GT_BOX: False
    USE_GT_OBJECT_LABEL: False
    REQUIRE_BOX_OVERLAP: False              # for sgdet, during training, only train pairs with overlap
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: True    # for sgdet only, in case some gt boxes are missing
    NUM_CLASSES: 51                 # 51 for VG, 201 for GQA (not contain "to the left of" & "to the right of")
    BATCH_SIZE_PER_IMAGE: 1024      # sample as much as possible
    POSITIVE_FRACTION: 0.25
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_HIDDEN_DIM: 512         #1024 for VCTree
    POOLING_ALL_LEVELS: True
    LABEL_SMOOTHING_LOSS: False
    FEATURE_EXTRACTOR: "RelationFeatureExtractor"
    #################### Select Relationship Model ####################
    #PREDICTOR: "MotifPredictor"
    #PREDICTOR: "VCTreePredictor"
    PREDICTOR: "TransformerPredictor"
#    PREDICTOR: "CausalAnalysisPredictor"
    ################# Parameters for Motif Predictor ##################
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_REL_LAYER: 1
    ############# Parameters for Causal Unbias Predictor ##############
    ### Implementation for paper "Unbiased Scene Graph Generation from Biased Training"
    CAUSAL:
      EFFECT_TYPE: 'none'             # candicates: 'TDE', 'NIE', 'TE', 'none'
      FUSION_TYPE: 'sum'              # candicates: 'sum', 'gate'         
      SEPARATE_SPATIAL: False         # separate spatial in union feature
      CONTEXT_LAYER: "motifs"         # candicates: motifs, vctree, vtranse
      SPATIAL_FOR_VISION: True
      EFFECT_ANALYSIS: True
    ############### Parameters for Transformer Predictor ##############
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      USE_GRAPH_EMBEDDING: False
      USE_GRAPH_MASK: False
      GRAPH_EBD:
        ENCODE_LAYER: GCN
        GRAPH_BIAS: one
        GAT:
          DROPOUT: 0.1
          HID_DIM: 128
          LEAKY_RELU_ALPHA: 0.001
          NUM_HEAD: 8
        GCN:
          DROPOUT: 0.1
          HID_DIM: 128
      OBJ_LAYER: 4
      REL_LAYER: 2
      NUM_HEAD: 8
      KEY_DIM: 64
      VAL_DIM: 64
      INNER_DIM: 2048
DATASETS:
  TRAIN: ("VG_stanford_filtered_with_attribute_train",)
  VAL: ("VG_stanford_filtered_with_attribute_val",)
  TEST: ("VG_stanford_filtered_with_attribute_test",)
DATALOADER:
  SIZE_DIVISIBILITY: 32
SOLVER:
  BIAS_LR_FACTOR: 1
  BASE_LR: 0.001
  # change when batch size change
  IMS_PER_BATCH: 16
  MAX_ITER: 16000
  WARMUP_ITERS: 500
  STEPS: (10000, 16000)
  # end change
  WARMUP_FACTOR: 0.1
  WARMUP_METHOD: linear
  GAMMA: 0.1
  WEIGHT_DECAY: 0.0001
  MOMENTUM: 0.9
  GRAD_NORM_CLIP: 5.0
  VAL_PERIOD: 2000
  CHECKPOINT_PERIOD: 4000
  PRINT_GRAD_FREQ: 4000
  SCHEDULE:
    # the following paramters are only used for WarmupReduceLROnPlateau
    TYPE: "WarmupMultiStepLR"    # WarmupMultiStepLR, WarmupReduceLROnPlateau
    PATIENCE: 2
    THRESHOLD: 0.001
    COOLDOWN: 0
    FACTOR: 0.1
    MAX_DECAY_STEP: 5

OUTPUT_DIR: './output/relation_baseline'
TEST:
  IMS_PER_BATCH: 4
  ALLOW_LOAD_FROM_CACHE: False
  RELATION:
    SYNC_GATHER: True      # turn on will slow down the evaluation to solve the sgdet test out of memory problem
    REQUIRE_OVERLAP: False
    LATER_NMS_PREDICTION_THRES: 0.5
  CUSTUM_EVAL: False       # eval SGDet model on custum images, output a json
  CUSTUM_PATH: '.'         # the folder that contains the custum images, only jpg files are allowed  

2022-04-25 16:07:33,137 maskrcnn_benchmark INFO: Running with config:
AMP_VERBOSE: False
CONCEPTNET_DIR: /public/data1/users/chenchao278/data/conceptNet/
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  NUM_WORKERS: 0
  SIZE_DIVISIBILITY: 32
DATASETS:
  TEST: ('VG_stanford_filtered_with_attribute_test',)
  TRAIN: ('VG_stanford_filtered_with_attribute_train',)
  VAL: ('VG_stanford_filtered_with_attribute_val',)
DEBUG: False
DETECTED_SGG_DIR: .
DTYPE: float32
GLOVE_DIR: glove
INPUT:
  BRIGHTNESS: 0.0
  CONTRAST: 0.0
  HUE: 0.0
  MAX_SIZE_TEST: 1000
  MAX_SIZE_TRAIN: 1000
  MIN_SIZE_TEST: 600
  MIN_SIZE_TRAIN: (600,)
  PIXEL_MEAN: [102.9801, 115.9465, 122.7717]
  PIXEL_STD: [1.0, 1.0, 1.0]
  SATURATION: 0.0
  TO_BGR255: True
  VERTICAL_FLIP_PROB_TRAIN: 0.0
MODEL:
  ATTRIBUTE_ON: False
  BACKBONE:
    CONV_BODY: R-101-FPN
    FREEZE_CONV_BODY_AT: 2
  CLS_AGNOSTIC_BBOX_REG: False
  DEVICE: cuda
  FBNET:
    ARCH: default
    ARCH_DEF: 
    BN_TYPE: bn
    DET_HEAD_BLOCKS: []
    DET_HEAD_LAST_SCALE: 1.0
    DET_HEAD_STRIDE: 0
    DW_CONV_SKIP_BN: True
    DW_CONV_SKIP_RELU: True
    KPTS_HEAD_BLOCKS: []
    KPTS_HEAD_LAST_SCALE: 0.0
    KPTS_HEAD_STRIDE: 0
    MASK_HEAD_BLOCKS: []
    MASK_HEAD_LAST_SCALE: 0.0
    MASK_HEAD_STRIDE: 0
    RPN_BN_TYPE: 
    RPN_HEAD_BLOCKS: 0
    SCALE_FACTOR: 1.0
    WIDTH_DIVISOR: 1
  FLIP_AUG: False
  FPN:
    USE_GN: False
    USE_RELU: False
  GROUP_NORM:
    DIM_PER_GP: -1
    EPSILON: 1e-05
    NUM_GROUPS: 32
  KEYPOINT_ON: False
  MASK_ON: False
  META_ARCHITECTURE: GeneralizedRCNN
  PRETRAINED_DETECTOR_CKPT: vg_faster_det.pth
  RELATION_ON: True
  RESNETS:
    BACKBONE_OUT_CHANNELS: 256
    DEFORMABLE_GROUPS: 1
    NUM_GROUPS: 32
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STAGE_WITH_DCN: (False, False, False, False)
    STEM_FUNC: StemWithFixedBatchNorm
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: False
    TRANS_FUNC: BottleneckWithFixedBatchNorm
    WIDTH_PER_GROUP: 8
    WITH_MODULATED_DCN: False
  RETINANET:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDES: (8, 16, 32, 64, 128)
    ASPECT_RATIOS: (0.5, 1.0, 2.0)
    BBOX_REG_BETA: 0.11
    BBOX_REG_WEIGHT: 4.0
    BG_IOU_THRESHOLD: 0.4
    FG_IOU_THRESHOLD: 0.5
    INFERENCE_TH: 0.05
    LOSS_ALPHA: 0.25
    LOSS_GAMMA: 2.0
    NMS_TH: 0.4
    NUM_CLASSES: 81
    NUM_CONVS: 4
    OCTAVE: 2.0
    PRE_NMS_TOP_N: 1000
    PRIOR_PROB: 0.01
    SCALES_PER_OCTAVE: 3
    STRADDLE_THRESH: 0
    USE_C5: True
  RETINANET_ON: False
  ROI_ATTRIBUTE_HEAD:
    ATTRIBUTE_BGFG_RATIO: 3
    ATTRIBUTE_BGFG_SAMPLE: True
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MAX_ATTRIBUTES: 10
    NUM_ATTRIBUTES: 201
    POS_WEIGHT: 50.0
    PREDICTOR: FPNPredictor
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_BINARY_LOSS: True
  ROI_BOX_HEAD:
    CONV_HEAD_DIM: 256
    DILATION: 1
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MLP_HEAD_DIM: 4096
    NUM_CLASSES: 151
    NUM_STACKED_CONVS: 4
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 2
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    PREDICTOR: FPNPredictor
    USE_GN: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    BG_IOU_THRESHOLD: 0.3
    DETECTIONS_PER_IMG: 80
    FG_IOU_THRESHOLD: 0.5
    NMS: 0.3
    NMS_FILTER_DUPLICATES: True
    POSITIVE_FRACTION: 0.5
    POST_NMS_PER_CLS_TOPN: 300
    SCORE_THRESH: 0.01
    USE_FPN: True
  ROI_KEYPOINT_HEAD:
    CONV_LAYERS: (512, 512, 512, 512, 512, 512, 512, 512)
    FEATURE_EXTRACTOR: KeypointRCNNFeatureExtractor
    MLP_HEAD_DIM: 1024
    NUM_CLASSES: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    PREDICTOR: KeypointRCNNPredictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
  ROI_MASK_HEAD:
    CONV_LAYERS: (256, 256, 256, 256)
    DILATION: 1
    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor
    MLP_HEAD_DIM: 1024
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    POSTPROCESS_MASKS: False
    POSTPROCESS_MASKS_THRESHOLD: 0.5
    PREDICTOR: MaskRCNNC4Predictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_GN: False
  ROI_RELATION_HEAD:
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: True
    BATCH_SIZE_PER_IMAGE: 1024
    BIAS_MODULE:
      DROPOUT: 0.0
      EPSILON: 0.001
      PENALTY_BIAS:
        BG_DEFAULT_VALUE: 0.02
        CB_CLS_FUSION_TYPE: sum
        CB_CLS_FUSION_WEIGHT: 0.8
        CLS_TRANS: none
        EVAL_WITH_PENALTY: False
        PENALTY_EPSILON: 0.001
        PENALTY_FUSION_WEIGHTS: [1.0, 1.0]
        PENALTY_K: 10
        PENALTY_THRESHOLD: 0.5
        PENALTY_TYPE: static
        PENALTY_WEIGHT: 0.1
        POSSIBLE_BIAS_DEFAULT_VALUE: 1.0
        POSSIBLE_BIAS_THRESHOLD: 100.0
        SCALE_WEIGHT: 1.0
        USE_NEG_PENALTY: False
        WEIGHT_PATH: 
      USE_PENALTY: False
    CAUSAL:
      CONTEXT_LAYER: motifs
      EFFECT_ANALYSIS: True
      EFFECT_TYPE: none
      FUSION_TYPE: sum
      SEPARATE_SPATIAL: False
      SPATIAL_FOR_VISION: True
    CONTEXT_DROPOUT_RATE: 0.2
    CONTEXT_HIDDEN_DIM: 512
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_REL_LAYER: 1
    DUAL_TRANS:
      EVAL_USE_FC: False
      GRAPH_ENCODE_STRATEGY: trans
      REMOVE_BIAS: False
      TRANSFORMER:
        DROPOUT_RATE: 0.1
        KEY_DIM: 64
        NUM_HEAD: 8
        REL_LAYER: 2
        VAL_DIM: 64
      USE_GRAPH_ENCODE: True
      USE_GTRANS_CONTEXT: True
      USE_REL_GRAPH: True
    EMBED_DIM: 200
    FEATURE_EXTRACTOR: RelationFeatureExtractor
    FOCAL_LOSS:
      ALPHA: 0.25
      GAMMA: 2.0
      SIZE_AVERAGE: True
    LABEL_SMOOTHING_LOSS: False
    LOSS_WEIGHT_PATH: 
    NUM_CLASSES: 51
    NUM_SAMPLE_PER_GT_REL: 4
    POOLING_ALL_LEVELS: True
    POSITIVE_FRACTION: 0.25
    PREDICTOR: DualTransPredictor
    PREDICT_USE_BIAS: True
    PREDICT_USE_VISION: True
    REL_PROP: [0.01858, 0.00057, 0.00051, 0.00109, 0.0015, 0.00489, 0.00432, 0.02913, 0.00245, 0.00121, 0.00404, 0.0011, 0.00132, 0.00172, 5e-05, 0.00242, 0.0005, 0.00048, 0.00208, 0.15608, 0.0265, 0.06091, 0.009, 0.00183, 0.00225, 0.0009, 0.00028, 0.00077, 0.04844, 0.08645, 0.31621, 0.00088, 0.00301, 0.00042, 0.00186, 0.001, 0.00027, 0.01012, 0.0001, 0.01286, 0.00647, 0.00084, 0.01077, 0.00132, 0.00069, 0.00376, 0.00214, 0.11424, 0.01205, 0.02958]
    REQUIRE_BOX_OVERLAP: False
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      GRAPH_EBD:
        ENCODE_LAYER: GCN
        GAT:
          DROPOUT: 0.1
          HID_DIM: 128
          LEAKY_RELU_ALPHA: 0.001
          NUM_HEAD: 8
        GCN:
          DROPOUT: 0.1
          HID_DIM: 128
        GRAPH_BIAS: one
      INNER_DIM: 2048
      KEY_DIM: 64
      NUM_HEAD: 8
      OBJ_LAYER: 4
      REL_LAYER: 2
      USE_GRAPH_EMBEDDING: False
      USE_GRAPH_MASK: False
      VAL_DIM: 64
    USE_FOCAL_LOSS: False
    USE_GT_BOX: False
    USE_GT_OBJECT_LABEL: False
  RPN:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)
    BATCH_SIZE_PER_IMAGE: 256
    BG_IOU_THRESHOLD: 0.3
    FG_IOU_THRESHOLD: 0.7
    FPN_POST_NMS_PER_BATCH: False
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    MIN_SIZE: 0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOP_N_TEST: 1000
    POST_NMS_TOP_N_TRAIN: 1000
    PRE_NMS_TOP_N_TEST: 6000
    PRE_NMS_TOP_N_TRAIN: 6000
    RPN_HEAD: SingleConvRPNHead
    RPN_MID_CHANNEL: 256
    STRADDLE_THRESH: 0
    USE_FPN: True
  RPN_ONLY: False
  VGG:
    VGG16_OUT_CHANNELS: 512
  WEIGHT: catalog://ImageNetPretrained/FAIR/20171220/X-101-32x8d
OUTPUT_DIR: dual
PATHS_CATALOG: /root/RTPB/maskrcnn_benchmark/config/paths_catalog.py
PATHS_DATA: /root/RTPB/maskrcnn_benchmark/config/../data/datasets
SOLVER:
  BASE_LR: 0.001
  BIAS_LR_FACTOR: 1
  CHECKPOINT_PERIOD: 4000
  CLIP_NORM: 5.0
  GAMMA: 0.1
  GRAD_NORM_CLIP: 5.0
  IMS_PER_BATCH: 16
  MAX_ITER: 18000
  MOMENTUM: 0.9
  PRE_VAL: False
  PRINT_GRAD_FREQ: 4000
  SCHEDULE:
    COOLDOWN: 0
    FACTOR: 0.1
    MAX_DECAY_STEP: 5
    PATIENCE: 2
    THRESHOLD: 0.001
    TYPE: WarmupMultiStepLR
  STEPS: (10000, 16000)
  TO_VAL: True
  UPDATE_SCHEDULE_DURING_LOAD: False
  VAL_PERIOD: 2000
  WARMUP_FACTOR: 0.1
  WARMUP_ITERS: 500
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0
TEST:
  ALLOW_LOAD_FROM_CACHE: False
  BBOX_AUG:
    ENABLED: False
    H_FLIP: False
    MAX_SIZE: 4000
    SCALES: ()
    SCALE_H_FLIP: False
  CUSTUM_EVAL: False
  CUSTUM_PATH: .
  DEBUG: False
  DETECTIONS_PER_IMG: 100
  EXPECTED_RESULTS: []
  EXPECTED_RESULTS_SIGMA_TOL: 4
  IMS_PER_BATCH: 2
  RELATION:
    IOU_THRESHOLD: 0.5
    LATER_NMS_PREDICTION_THRES: 0.5
    MULTIPLE_PREDS: False
    REQUIRE_OVERLAP: False
    SYNC_GATHER: True
  SAVE_PROPOSALS: False
2022-04-25 16:07:33,138 maskrcnn_benchmark INFO: Saving config into: dual/config.yml
2022-04-25 16:07:33,199 maskrcnn_benchmark INFO: #################### prepare training ####################
2022-04-25 16:07:37,759 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
2022-04-25 16:07:37,759 maskrcnn_benchmark.data.build INFO: get dataset statistics...
2022-04-25 16:07:37,760 maskrcnn_benchmark.data.build INFO: Loading data statistics from: dual/VG_stanford_filtered_with_attribute_train_statistics.cache
2022-04-25 16:07:37,760 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
2022-04-25 16:07:38,700 maskrcnn_benchmark INFO: #################### end model construction ####################
2022-04-25 16:07:38,717 maskrcnn_benchmark INFO: 
backbone.body.stem.conv1.weight                                                 : [64,3,7,7]      (    9408) (    )
backbone.body.layer1.0.downsample.0.weight                                      : [256,64,1,1]    (   16384) (    )
backbone.body.layer1.0.conv1.weight                                             : [256,64,1,1]    (   16384) (    )
backbone.body.layer1.0.conv2.weight                                             : [256,8,3,3]     (   18432) (    )
backbone.body.layer1.0.conv3.weight                                             : [256,256,1,1]   (   65536) (    )
backbone.body.layer1.1.conv1.weight                                             : [256,256,1,1]   (   65536) (    )
backbone.body.layer1.1.conv2.weight                                             : [256,8,3,3]     (   18432) (    )
backbone.body.layer1.1.conv3.weight                                             : [256,256,1,1]   (   65536) (    )
backbone.body.layer1.2.conv1.weight                                             : [256,256,1,1]   (   65536) (    )
backbone.body.layer1.2.conv2.weight                                             : [256,8,3,3]     (   18432) (    )
backbone.body.layer1.2.conv3.weight                                             : [256,256,1,1]   (   65536) (    )
backbone.body.layer2.0.downsample.0.weight                                      : [512,256,1,1]   (  131072) (    )
backbone.body.layer2.0.conv1.weight                                             : [512,256,1,1]   (  131072) (    )
backbone.body.layer2.0.conv2.weight                                             : [512,16,3,3]    (   73728) (    )
backbone.body.layer2.0.conv3.weight                                             : [512,512,1,1]   (  262144) (    )
backbone.body.layer2.1.conv1.weight                                             : [512,512,1,1]   (  262144) (    )
backbone.body.layer2.1.conv2.weight                                             : [512,16,3,3]    (   73728) (    )
backbone.body.layer2.1.conv3.weight                                             : [512,512,1,1]   (  262144) (    )
backbone.body.layer2.2.conv1.weight                                             : [512,512,1,1]   (  262144) (    )
backbone.body.layer2.2.conv2.weight                                             : [512,16,3,3]    (   73728) (    )
backbone.body.layer2.2.conv3.weight                                             : [512,512,1,1]   (  262144) (    )
backbone.body.layer2.3.conv1.weight                                             : [512,512,1,1]   (  262144) (    )
backbone.body.layer2.3.conv2.weight                                             : [512,16,3,3]    (   73728) (    )
backbone.body.layer2.3.conv3.weight                                             : [512,512,1,1]   (  262144) (    )
backbone.body.layer3.0.downsample.0.weight                                      : [1024,512,1,1]  (  524288) (    )
backbone.body.layer3.0.conv1.weight                                             : [1024,512,1,1]  (  524288) (    )
backbone.body.layer3.0.conv2.weight                                             : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.0.conv3.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.1.conv1.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.1.conv2.weight                                             : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.1.conv3.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.2.conv1.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.2.conv2.weight                                             : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.2.conv3.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.3.conv1.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.3.conv2.weight                                             : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.3.conv3.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.4.conv1.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.4.conv2.weight                                             : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.4.conv3.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.5.conv1.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.5.conv2.weight                                             : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.5.conv3.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.6.conv1.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.6.conv2.weight                                             : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.6.conv3.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.7.conv1.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.7.conv2.weight                                             : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.7.conv3.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.8.conv1.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.8.conv2.weight                                             : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.8.conv3.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.9.conv1.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.9.conv2.weight                                             : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.9.conv3.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.10.conv1.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.10.conv2.weight                                            : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.10.conv3.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.11.conv1.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.11.conv2.weight                                            : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.11.conv3.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.12.conv1.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.12.conv2.weight                                            : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.12.conv3.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.13.conv1.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.13.conv2.weight                                            : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.13.conv3.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.14.conv1.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.14.conv2.weight                                            : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.14.conv3.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.15.conv1.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.15.conv2.weight                                            : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.15.conv3.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.16.conv1.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.16.conv2.weight                                            : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.16.conv3.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.17.conv1.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.17.conv2.weight                                            : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.17.conv3.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.18.conv1.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.18.conv2.weight                                            : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.18.conv3.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.19.conv1.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.19.conv2.weight                                            : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.19.conv3.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.20.conv1.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.20.conv2.weight                                            : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.20.conv3.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.21.conv1.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.21.conv2.weight                                            : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.21.conv3.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.22.conv1.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.22.conv2.weight                                            : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.22.conv3.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer4.0.downsample.0.weight                                      : [2048,1024,1,1] ( 2097152) (    )
backbone.body.layer4.0.conv1.weight                                             : [2048,1024,1,1] ( 2097152) (    )
backbone.body.layer4.0.conv2.weight                                             : [2048,64,3,3]   ( 1179648) (    )
backbone.body.layer4.0.conv3.weight                                             : [2048,2048,1,1] ( 4194304) (    )
backbone.body.layer4.1.conv1.weight                                             : [2048,2048,1,1] ( 4194304) (    )
backbone.body.layer4.1.conv2.weight                                             : [2048,64,3,3]   ( 1179648) (    )
backbone.body.layer4.1.conv3.weight                                             : [2048,2048,1,1] ( 4194304) (    )
backbone.body.layer4.2.conv1.weight                                             : [2048,2048,1,1] ( 4194304) (    )
backbone.body.layer4.2.conv2.weight                                             : [2048,64,3,3]   ( 1179648) (    )
backbone.body.layer4.2.conv3.weight                                             : [2048,2048,1,1] ( 4194304) (    )
backbone.fpn.fpn_inner1.weight                                                  : [256,256,1,1]   (   65536) (    )
backbone.fpn.fpn_layer1.weight                                                  : [256,256,3,3]   (  589824) (    )
backbone.fpn.fpn_inner2.weight                                                  : [256,512,1,1]   (  131072) (    )
backbone.fpn.fpn_layer2.weight                                                  : [256,256,3,3]   (  589824) (    )
backbone.fpn.fpn_inner3.weight                                                  : [256,1024,1,1]  (  262144) (    )
backbone.fpn.fpn_layer3.weight                                                  : [256,256,3,3]   (  589824) (    )
backbone.fpn.fpn_inner4.weight                                                  : [256,2048,1,1]  (  524288) (    )
backbone.fpn.fpn_layer4.weight                                                  : [256,256,3,3]   (  589824) (    )
rpn.head.conv.weight                                                            : [256,256,3,3]   (  589824) (    )
rpn.head.cls_logits.weight                                                      : [4,256,1,1]     (    1024) (    )
rpn.head.bbox_pred.weight                                                       : [16,256,1,1]    (    4096) (    )
roi_heads.box.feature_extractor.fc6.weight                                      : [4096,12544]    (51380224) (    )
roi_heads.box.feature_extractor.fc7.weight                                      : [4096,4096]     (16777216) (    )
roi_heads.box.predictor.cls_score.weight                                        : [151,4096]      (  618496) (    )
roi_heads.box.predictor.bbox_pred.weight                                        : [604,4096]      ( 2473984) (    )
roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: [256,1024,3,3]  ( 2359296) (grad)
roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight         : [4096,12544]    (51380224) (grad)
roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight         : [4096,4096]     (16777216) (grad)
roi_heads.relation.union_feature_extractor.rect_conv.0.weight                   : [128,2,7,7]     (   12544) (grad)
roi_heads.relation.union_feature_extractor.rect_conv.2.weight                   : [128]           (     128) (grad)
roi_heads.relation.union_feature_extractor.rect_conv.4.weight                   : [256,128,3,3]   (  294912) (grad)
roi_heads.relation.union_feature_extractor.rect_conv.6.weight                   : [256]           (     256) (grad)
roi_heads.relation.box_feature_extractor.fc6.weight                             : [4096,12544]    (51380224) (grad)
roi_heads.relation.box_feature_extractor.fc7.weight                             : [4096,4096]     (16777216) (grad)
roi_heads.relation.predictor.context_layer.obj_embed1.weight                    : [151,200]       (   30200) (grad)
roi_heads.relation.predictor.context_layer.obj_embed2.weight                    : [151,200]       (   30200) (grad)
roi_heads.relation.predictor.context_layer.bbox_embed.0.weight                  : [32,9]          (     288) (grad)
roi_heads.relation.predictor.context_layer.bbox_embed.3.weight                  : [128,32]        (    4096) (grad)
roi_heads.relation.predictor.context_layer.lin_obj.weight                       : [512,4424]      ( 2265088) (grad)
roi_heads.relation.predictor.context_layer.out_obj.weight                       : [151,512]       (   77312) (grad)
roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_qs.weight: [512,512]       (  262144) (grad)
roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_ks.weight: [512,512]       (  262144) (grad)
roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_vs.weight: [512,512]       (  262144) (grad)
roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.layer_norm.weight: [512]           (     512) (grad)
roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.fc.weight: [512,512]       (  262144) (grad)
roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_1.weight: [2048,512,1]    ( 1048576) (grad)
roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_2.weight: [512,2048,1]    ( 1048576) (grad)
roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.layer_norm.weight: [512]           (     512) (grad)
roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_qs.weight: [512,512]       (  262144) (grad)
roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_ks.weight: [512,512]       (  262144) (grad)
roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_vs.weight: [512,512]       (  262144) (grad)
roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.layer_norm.weight: [512]           (     512) (grad)
roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.fc.weight: [512,512]       (  262144) (grad)
roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_1.weight: [2048,512,1]    ( 1048576) (grad)
roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_2.weight: [512,2048,1]    ( 1048576) (grad)
roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.layer_norm.weight: [512]           (     512) (grad)
roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_qs.weight: [512,512]       (  262144) (grad)
roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_ks.weight: [512,512]       (  262144) (grad)
roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_vs.weight: [512,512]       (  262144) (grad)
roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.layer_norm.weight: [512]           (     512) (grad)
roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.fc.weight: [512,512]       (  262144) (grad)
roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_1.weight: [2048,512,1]    ( 1048576) (grad)
roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_2.weight: [512,2048,1]    ( 1048576) (grad)
roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.layer_norm.weight: [512]           (     512) (grad)
roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_qs.weight: [512,512]       (  262144) (grad)
roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_ks.weight: [512,512]       (  262144) (grad)
roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_vs.weight: [512,512]       (  262144) (grad)
roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.layer_norm.weight: [512]           (     512) (grad)
roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.fc.weight: [512,512]       (  262144) (grad)
roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_1.weight: [2048,512,1]    ( 1048576) (grad)
roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_2.weight: [512,2048,1]    ( 1048576) (grad)
roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.layer_norm.weight: [512]           (     512) (grad)
roi_heads.relation.predictor.post_obj_edge_repr.weight                          : [1024,512]      (  524288) (grad)
roi_heads.relation.predictor.pred_up_dim.weight                                 : [4096,1024]     ( 4194304) (grad)
roi_heads.relation.predictor.mix_ctx.weight                                     : [1024,8192]     ( 8388608) (grad)
roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.w_qs.weight: [512,1024]      (  524288) (grad)
roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.w_ks.weight: [512,1024]      (  524288) (grad)
roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.w_vs.weight: [512,1024]      (  524288) (grad)
roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.layer_norm.weight: [1024]          (    1024) (grad)
roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.fc.weight: [1024,512]      (  524288) (grad)
roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.pos_ffn.w_1.weight: [1024,1024,1]   ( 1048576) (grad)
roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.pos_ffn.w_2.weight: [1024,1024,1]   ( 1048576) (grad)
roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.pos_ffn.layer_norm.weight: [1024]          (    1024) (grad)
roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.w_qs.weight: [512,1024]      (  524288) (grad)
roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.w_ks.weight: [512,1024]      (  524288) (grad)
roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.w_vs.weight: [512,1024]      (  524288) (grad)
roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.layer_norm.weight: [1024]          (    1024) (grad)
roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.fc.weight: [1024,512]      (  524288) (grad)
roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.pos_ffn.w_1.weight: [1024,1024,1]   ( 1048576) (grad)
roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.pos_ffn.w_2.weight: [1024,1024,1]   ( 1048576) (grad)
roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.pos_ffn.layer_norm.weight: [1024]          (    1024) (grad)
roi_heads.relation.predictor.rel_visual_clf.weight                              : [51,4096]       (  208896) (grad)
roi_heads.relation.predictor.rel_clf.weight                                     : [51,1024]       (   52224) (grad)
roi_heads.relation.predictor.post_rel2ctx.weight                                : [4096,1024]     ( 4194304) (grad)
roi_heads.relation.predictor.bias_module.bias_module.obj_baseline.weight        : [22801,51]      ( 1162851) (grad)
 ----- 
 
      trainable parameters:  181.159/342.897 M 
 
2022-04-25 16:07:39,289 maskrcnn_benchmark INFO: #################### end optimizer and shcedule ####################
2022-04-25 16:07:39,605 maskrcnn_benchmark INFO: #################### end distributed ####################
2022-04-25 16:07:39,608 maskrcnn_benchmark.utils.checkpoint INFO: Loading checkpoint from vg_faster_det.pth
2022-04-25 16:07:40,699 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING module.roi_heads.relation.box_feature_extractor.fc6.bias in current model to module.roi_heads.box.feature_extractor.fc6.bias in loaded model.
2022-04-25 16:07:40,699 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING module.roi_heads.relation.box_feature_extractor.fc6.weight in current model to module.roi_heads.box.feature_extractor.fc6.weight in loaded model.
2022-04-25 16:07:40,699 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING module.roi_heads.relation.box_feature_extractor.fc7.bias in current model to module.roi_heads.box.feature_extractor.fc7.bias in loaded model.
2022-04-25 16:07:40,699 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING module.roi_heads.relation.box_feature_extractor.fc7.weight in current model to module.roi_heads.box.feature_extractor.fc7.weight in loaded model.
2022-04-25 16:07:40,699 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING module.roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias in current model to module.roi_heads.box.feature_extractor.fc6.bias in loaded model.
2022-04-25 16:07:40,699 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING module.roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight in current model to module.roi_heads.box.feature_extractor.fc6.weight in loaded model.
2022-04-25 16:07:40,699 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING module.roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias in current model to module.roi_heads.box.feature_extractor.fc7.bias in loaded model.
2022-04-25 16:07:40,700 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING module.roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight in current model to module.roi_heads.box.feature_extractor.fc7.weight in loaded model.
2022-04-25 16:07:40,700 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING module.roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias in current model to module.roi_heads.box.feature_extractor.pooler.reduce_channel.0.bias in loaded model.
2022-04-25 16:07:40,700 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING module.roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight in current model to module.roi_heads.box.feature_extractor.pooler.reduce_channel.0.weight in loaded model.
2022-04-25 16:07:40,836 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! module.roi_heads.relation.box_feature_extractor.fc6.bias                                                   loaded from roi_heads.box.feature_extractor.fc6.bias         of shape (4096,)
2022-04-25 16:07:40,836 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! module.roi_heads.relation.box_feature_extractor.fc6.weight                                                 loaded from roi_heads.box.feature_extractor.fc6.weight       of shape (4096, 12544)
2022-04-25 16:07:40,836 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! module.roi_heads.relation.box_feature_extractor.fc7.bias                                                   loaded from roi_heads.box.feature_extractor.fc7.bias         of shape (4096,)
2022-04-25 16:07:40,836 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! module.roi_heads.relation.box_feature_extractor.fc7.weight                                                 loaded from roi_heads.box.feature_extractor.fc7.weight       of shape (4096, 4096)
2022-04-25 16:07:40,837 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.bias_module.bias_module.obj_baseline.weight of shape (22801, 51)
2022-04-25 16:07:40,837 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.bbox_embed.0.bias of shape (32,)
2022-04-25 16:07:40,837 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.bbox_embed.0.weight of shape (32, 9)
2022-04-25 16:07:40,837 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.bbox_embed.3.bias of shape (128,)
2022-04-25 16:07:40,837 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.bbox_embed.3.weight of shape (128, 32)
2022-04-25 16:07:40,837 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.layer_norm.bias of shape (512,)
2022-04-25 16:07:40,837 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.layer_norm.weight of shape (512,)
2022-04-25 16:07:40,837 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_1.bias of shape (2048,)
2022-04-25 16:07:40,837 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_1.weight of shape (2048, 512, 1)
2022-04-25 16:07:40,837 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_2.bias of shape (512,)
2022-04-25 16:07:40,837 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_2.weight of shape (512, 2048, 1)
2022-04-25 16:07:40,837 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.fc.bias of shape (512,)
2022-04-25 16:07:40,837 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.fc.weight of shape (512, 512)
2022-04-25 16:07:40,838 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.layer_norm.bias of shape (512,)
2022-04-25 16:07:40,838 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.layer_norm.weight of shape (512,)
2022-04-25 16:07:40,838 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_ks.bias of shape (512,)
2022-04-25 16:07:40,838 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_ks.weight of shape (512, 512)
2022-04-25 16:07:40,838 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_qs.bias of shape (512,)
2022-04-25 16:07:40,838 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_qs.weight of shape (512, 512)
2022-04-25 16:07:40,838 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_vs.bias of shape (512,)
2022-04-25 16:07:40,838 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_vs.weight of shape (512, 512)
2022-04-25 16:07:40,838 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.layer_norm.bias of shape (512,)
2022-04-25 16:07:40,838 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.layer_norm.weight of shape (512,)
2022-04-25 16:07:40,838 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_1.bias of shape (2048,)
2022-04-25 16:07:40,838 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_1.weight of shape (2048, 512, 1)
2022-04-25 16:07:40,838 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_2.bias of shape (512,)
2022-04-25 16:07:40,838 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_2.weight of shape (512, 2048, 1)
2022-04-25 16:07:40,839 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.fc.bias of shape (512,)
2022-04-25 16:07:40,839 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.fc.weight of shape (512, 512)
2022-04-25 16:07:40,839 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.layer_norm.bias of shape (512,)
2022-04-25 16:07:40,839 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.layer_norm.weight of shape (512,)
2022-04-25 16:07:40,839 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_ks.bias of shape (512,)
2022-04-25 16:07:40,839 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_ks.weight of shape (512, 512)
2022-04-25 16:07:40,839 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_qs.bias of shape (512,)
2022-04-25 16:07:40,839 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_qs.weight of shape (512, 512)
2022-04-25 16:07:40,839 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_vs.bias of shape (512,)
2022-04-25 16:07:40,839 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_vs.weight of shape (512, 512)
2022-04-25 16:07:40,839 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.layer_norm.bias of shape (512,)
2022-04-25 16:07:40,839 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.layer_norm.weight of shape (512,)
2022-04-25 16:07:40,839 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_1.bias of shape (2048,)
2022-04-25 16:07:40,840 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_1.weight of shape (2048, 512, 1)
2022-04-25 16:07:40,840 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_2.bias of shape (512,)
2022-04-25 16:07:40,840 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_2.weight of shape (512, 2048, 1)
2022-04-25 16:07:40,840 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.fc.bias of shape (512,)
2022-04-25 16:07:40,840 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.fc.weight of shape (512, 512)
2022-04-25 16:07:40,840 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.layer_norm.bias of shape (512,)
2022-04-25 16:07:40,840 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.layer_norm.weight of shape (512,)
2022-04-25 16:07:40,840 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_ks.bias of shape (512,)
2022-04-25 16:07:40,840 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_ks.weight of shape (512, 512)
2022-04-25 16:07:40,840 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_qs.bias of shape (512,)
2022-04-25 16:07:40,840 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_qs.weight of shape (512, 512)
2022-04-25 16:07:40,840 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_vs.bias of shape (512,)
2022-04-25 16:07:40,841 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_vs.weight of shape (512, 512)
2022-04-25 16:07:40,841 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.layer_norm.bias of shape (512,)
2022-04-25 16:07:40,841 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.layer_norm.weight of shape (512,)
2022-04-25 16:07:40,841 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_1.bias of shape (2048,)
2022-04-25 16:07:40,841 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_1.weight of shape (2048, 512, 1)
2022-04-25 16:07:40,841 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_2.bias of shape (512,)
2022-04-25 16:07:40,841 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_2.weight of shape (512, 2048, 1)
2022-04-25 16:07:40,841 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.fc.bias of shape (512,)
2022-04-25 16:07:40,841 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.fc.weight of shape (512, 512)
2022-04-25 16:07:40,841 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.layer_norm.bias of shape (512,)
2022-04-25 16:07:40,841 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.layer_norm.weight of shape (512,)
2022-04-25 16:07:40,841 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_ks.bias of shape (512,)
2022-04-25 16:07:40,841 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_ks.weight of shape (512, 512)
2022-04-25 16:07:40,842 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_qs.bias of shape (512,)
2022-04-25 16:07:40,842 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_qs.weight of shape (512, 512)
2022-04-25 16:07:40,842 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_vs.bias of shape (512,)
2022-04-25 16:07:40,842 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_vs.weight of shape (512, 512)
2022-04-25 16:07:40,842 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.lin_obj.bias of shape (512,)
2022-04-25 16:07:40,842 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.lin_obj.weight of shape (512, 4424)
2022-04-25 16:07:40,842 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.obj_embed1.weight of shape (151, 200)
2022-04-25 16:07:40,842 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.obj_embed2.weight of shape (151, 200)
2022-04-25 16:07:40,842 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.out_obj.bias of shape (151,)
2022-04-25 16:07:40,842 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.context_layer.out_obj.weight of shape (151, 512)
2022-04-25 16:07:40,842 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.pos_ffn.layer_norm.bias of shape (1024,)
2022-04-25 16:07:40,842 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.pos_ffn.layer_norm.weight of shape (1024,)
2022-04-25 16:07:40,842 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.pos_ffn.w_1.bias of shape (1024,)
2022-04-25 16:07:40,842 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.pos_ffn.w_1.weight of shape (1024, 1024, 1)
2022-04-25 16:07:40,843 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.pos_ffn.w_2.bias of shape (1024,)
2022-04-25 16:07:40,843 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.pos_ffn.w_2.weight of shape (1024, 1024, 1)
2022-04-25 16:07:40,843 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.fc.bias of shape (1024,)
2022-04-25 16:07:40,843 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.fc.weight of shape (1024, 512)
2022-04-25 16:07:40,843 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.layer_norm.bias of shape (1024,)
2022-04-25 16:07:40,843 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.layer_norm.weight of shape (1024,)
2022-04-25 16:07:40,843 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.w_ks.bias of shape (512,)
2022-04-25 16:07:40,843 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.w_ks.weight of shape (512, 1024)
2022-04-25 16:07:40,843 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.w_qs.bias of shape (512,)
2022-04-25 16:07:40,843 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.w_qs.weight of shape (512, 1024)
2022-04-25 16:07:40,843 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.w_vs.bias of shape (512,)
2022-04-25 16:07:40,843 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.w_vs.weight of shape (512, 1024)
2022-04-25 16:07:40,843 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.pos_ffn.layer_norm.bias of shape (1024,)
2022-04-25 16:07:40,844 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.pos_ffn.layer_norm.weight of shape (1024,)
2022-04-25 16:07:40,844 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.pos_ffn.w_1.bias of shape (1024,)
2022-04-25 16:07:40,844 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.pos_ffn.w_1.weight of shape (1024, 1024, 1)
2022-04-25 16:07:40,844 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.pos_ffn.w_2.bias of shape (1024,)
2022-04-25 16:07:40,844 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.pos_ffn.w_2.weight of shape (1024, 1024, 1)
2022-04-25 16:07:40,844 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.fc.bias of shape (1024,)
2022-04-25 16:07:40,844 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.fc.weight of shape (1024, 512)
2022-04-25 16:07:40,844 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.layer_norm.bias of shape (1024,)
2022-04-25 16:07:40,844 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.layer_norm.weight of shape (1024,)
2022-04-25 16:07:40,844 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.w_ks.bias of shape (512,)
2022-04-25 16:07:40,844 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.w_ks.weight of shape (512, 1024)
2022-04-25 16:07:40,844 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.w_qs.bias of shape (512,)
2022-04-25 16:07:40,844 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.w_qs.weight of shape (512, 1024)
2022-04-25 16:07:40,844 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.w_vs.bias of shape (512,)
2022-04-25 16:07:40,845 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.w_vs.weight of shape (512, 1024)
2022-04-25 16:07:40,845 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.mix_ctx.bias of shape (1024,)
2022-04-25 16:07:40,845 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.mix_ctx.weight of shape (1024, 8192)
2022-04-25 16:07:40,845 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.post_obj_edge_repr.bias of shape (1024,)
2022-04-25 16:07:40,845 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.post_obj_edge_repr.weight of shape (1024, 512)
2022-04-25 16:07:40,845 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.post_rel2ctx.bias of shape (4096,)
2022-04-25 16:07:40,845 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.post_rel2ctx.weight of shape (4096, 1024)
2022-04-25 16:07:40,845 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.pred_up_dim.bias of shape (4096,)
2022-04-25 16:07:40,845 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.pred_up_dim.weight of shape (4096, 1024)
2022-04-25 16:07:40,845 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.rel_clf.bias of shape (51,)
2022-04-25 16:07:40,845 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.rel_clf.weight of shape (51, 1024)
2022-04-25 16:07:40,845 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.rel_visual_clf.bias of shape (51,)
2022-04-25 16:07:40,846 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.predictor.rel_visual_clf.weight of shape (51, 4096)
2022-04-25 16:07:40,846 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! module.roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias                               loaded from roi_heads.box.feature_extractor.fc6.bias         of shape (4096,)
2022-04-25 16:07:40,846 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! module.roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight                             loaded from roi_heads.box.feature_extractor.fc6.weight       of shape (4096, 12544)
2022-04-25 16:07:40,846 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! module.roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias                               loaded from roi_heads.box.feature_extractor.fc7.bias         of shape (4096,)
2022-04-25 16:07:40,846 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! module.roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight                             loaded from roi_heads.box.feature_extractor.fc7.weight       of shape (4096, 4096)
2022-04-25 16:07:40,846 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias of shape (256,)
2022-04-25 16:07:40,846 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight of shape (256, 1024, 3, 3)
2022-04-25 16:07:40,846 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.union_feature_extractor.rect_conv.0.bias of shape (128,)
2022-04-25 16:07:40,846 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.union_feature_extractor.rect_conv.0.weight of shape (128, 2, 7, 7)
2022-04-25 16:07:40,846 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.union_feature_extractor.rect_conv.2.bias of shape (128,)
2022-04-25 16:07:40,846 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.union_feature_extractor.rect_conv.2.num_batches_tracked of shape ()
2022-04-25 16:07:40,846 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.union_feature_extractor.rect_conv.2.running_mean of shape (128,)
2022-04-25 16:07:40,847 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.union_feature_extractor.rect_conv.2.running_var of shape (128,)
2022-04-25 16:07:40,847 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.union_feature_extractor.rect_conv.2.weight of shape (128,)
2022-04-25 16:07:40,847 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.union_feature_extractor.rect_conv.4.bias of shape (256,)
2022-04-25 16:07:40,847 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.union_feature_extractor.rect_conv.4.weight of shape (256, 128, 3, 3)
2022-04-25 16:07:40,847 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.union_feature_extractor.rect_conv.6.bias of shape (256,)
2022-04-25 16:07:40,847 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.union_feature_extractor.rect_conv.6.num_batches_tracked of shape ()
2022-04-25 16:07:40,847 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.union_feature_extractor.rect_conv.6.running_mean of shape (256,)
2022-04-25 16:07:40,847 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.union_feature_extractor.rect_conv.6.running_var of shape (256,)
2022-04-25 16:07:40,847 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: module.roi_heads.relation.union_feature_extractor.rect_conv.6.weight of shape (256,)
2022-04-25 16:07:41,274 maskrcnn_benchmark INFO: #################### end load checkpointer ####################
2022-04-25 16:07:41,274 maskrcnn_benchmark.data.build WARNING: When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
2022-04-25 16:07:46,479 maskrcnn_benchmark.utils.miscellaneous INFO: Saving labels mapping into dual/labels.json
2022-04-25 16:07:48,438 maskrcnn_benchmark INFO: #################### end dataloader ####################
2022-04-25 16:07:48,438 maskrcnn_benchmark INFO: Start training
2022-04-25 16:07:56,763 maskrcnn_benchmark INFO: ---Total norm 1564.57141 clip coef 0.00320-----------------
2022-04-25 16:07:56,813 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.mix_ctx.weight: 1225.37695, (torch.Size([1024, 8192]))
2022-04-25 16:07:56,813 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.pred_up_dim.weight: 436.63586, (torch.Size([4096, 1024]))
2022-04-25 16:07:56,813 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_2.weight: 397.21542, (torch.Size([512, 2048, 1]))
2022-04-25 16:07:56,814 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.lin_obj.weight: 312.00232, (torch.Size([512, 4424]))
2022-04-25 16:07:56,814 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_2.weight: 302.02298, (torch.Size([512, 2048, 1]))
2022-04-25 16:07:56,814 maskrcnn_benchmark INFO: module.roi_heads.relation.box_feature_extractor.fc7.weight: 266.34409, (torch.Size([4096, 4096]))
2022-04-25 16:07:56,814 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_2.weight: 256.53729, (torch.Size([512, 2048, 1]))
2022-04-25 16:07:56,814 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_2.weight: 229.32980, (torch.Size([512, 2048, 1]))
2022-04-25 16:07:56,814 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.rel_clf.weight: 181.23138, (torch.Size([51, 1024]))
2022-04-25 16:07:56,814 maskrcnn_benchmark INFO: module.roi_heads.relation.box_feature_extractor.fc6.weight: 169.02361, (torch.Size([4096, 12544]))
2022-04-25 16:07:56,814 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_1.weight: 159.83298, (torch.Size([2048, 512, 1]))
2022-04-25 16:07:56,814 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.fc.weight: 144.78079, (torch.Size([512, 512]))
2022-04-25 16:07:56,815 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_1.weight: 123.73554, (torch.Size([2048, 512, 1]))
2022-04-25 16:07:56,815 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_vs.weight: 108.34585, (torch.Size([512, 512]))
2022-04-25 16:07:56,815 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_1.weight: 106.63577, (torch.Size([2048, 512, 1]))
2022-04-25 16:07:56,815 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.fc.weight: 96.98348, (torch.Size([512, 512]))
2022-04-25 16:07:56,815 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_1.weight: 86.90723, (torch.Size([2048, 512, 1]))
2022-04-25 16:07:56,815 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_vs.weight: 73.18291, (torch.Size([512, 512]))
2022-04-25 16:07:56,815 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.fc.weight: 71.71991, (torch.Size([512, 512]))
2022-04-25 16:07:56,815 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.fc.weight: 68.42579, (torch.Size([512, 512]))
2022-04-25 16:07:56,815 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: 68.14255, (torch.Size([4096, 4096]))
2022-04-25 16:07:56,816 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: 67.66682, (torch.Size([4096, 12544]))
2022-04-25 16:07:56,816 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.pos_ffn.w_2.weight: 66.31977, (torch.Size([1024, 1024, 1]))
2022-04-25 16:07:56,816 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.pos_ffn.w_2.weight: 64.62836, (torch.Size([1024, 1024, 1]))
2022-04-25 16:07:56,816 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_vs.weight: 56.85707, (torch.Size([512, 512]))
2022-04-25 16:07:56,816 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_vs.weight: 52.38421, (torch.Size([512, 512]))
2022-04-25 16:07:56,816 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.pos_ffn.w_1.weight: 51.76696, (torch.Size([1024, 1024, 1]))
2022-04-25 16:07:56,816 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.pos_ffn.w_1.weight: 47.06457, (torch.Size([1024, 1024, 1]))
2022-04-25 16:07:56,816 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.post_rel2ctx.weight: 42.77284, (torch.Size([4096, 1024]))
2022-04-25 16:07:56,816 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.out_obj.weight: 40.07938, (torch.Size([151, 512]))
2022-04-25 16:07:56,817 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.rel_visual_clf.weight: 38.82888, (torch.Size([51, 4096]))
2022-04-25 16:07:56,817 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.fc.weight: 29.71735, (torch.Size([1024, 512]))
2022-04-25 16:07:56,817 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.fc.weight: 28.87564, (torch.Size([1024, 512]))
2022-04-25 16:07:56,817 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.lin_obj.bias: 26.54402, (torch.Size([512]))
2022-04-25 16:07:56,817 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.w_vs.weight: 25.50482, (torch.Size([512, 1024]))
2022-04-25 16:07:56,817 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.w_vs.weight: 24.00226, (torch.Size([512, 1024]))
2022-04-25 16:07:56,817 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_2.bias: 22.95903, (torch.Size([512]))
2022-04-25 16:07:56,817 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_vs.bias: 20.25095, (torch.Size([512]))
2022-04-25 16:07:56,817 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.fc.bias: 20.05102, (torch.Size([512]))
2022-04-25 16:07:56,818 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.post_obj_edge_repr.weight: 17.89574, (torch.Size([1024, 512]))
2022-04-25 16:07:56,818 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_ks.weight: 15.25235, (torch.Size([512, 512]))
2022-04-25 16:07:56,818 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: 15.01964, (torch.Size([256, 1024, 3, 3]))
2022-04-25 16:07:56,818 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_qs.weight: 13.79933, (torch.Size([512, 512]))
2022-04-25 16:07:56,818 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_ks.weight: 13.49545, (torch.Size([512, 512]))
2022-04-25 16:07:56,818 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_ks.weight: 13.26746, (torch.Size([512, 512]))
2022-04-25 16:07:56,818 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_qs.weight: 13.18396, (torch.Size([512, 512]))
2022-04-25 16:07:56,818 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_qs.weight: 12.99332, (torch.Size([512, 512]))
2022-04-25 16:07:56,818 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_2.bias: 11.28996, (torch.Size([512]))
2022-04-25 16:07:56,818 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_ks.weight: 9.07829, (torch.Size([512, 512]))
2022-04-25 16:07:56,819 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_qs.weight: 9.07690, (torch.Size([512, 512]))
2022-04-25 16:07:56,819 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.layer_norm.bias: 8.11952, (torch.Size([512]))
2022-04-25 16:07:56,819 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_1.bias: 8.01921, (torch.Size([2048]))
2022-04-25 16:07:56,819 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_2.bias: 7.66243, (torch.Size([512]))
2022-04-25 16:07:56,819 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.w_qs.weight: 7.53581, (torch.Size([512, 1024]))
2022-04-25 16:07:56,819 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: 7.38151, (torch.Size([128, 32]))
2022-04-25 16:07:56,819 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.w_ks.weight: 7.32454, (torch.Size([512, 1024]))
2022-04-25 16:07:56,819 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.rect_conv.4.weight: 6.80394, (torch.Size([256, 128, 3, 3]))
2022-04-25 16:07:56,819 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.layer_norm.bias: 6.62928, (torch.Size([512]))
2022-04-25 16:07:56,820 maskrcnn_benchmark INFO: module.roi_heads.relation.box_feature_extractor.fc7.bias: 6.38818, (torch.Size([4096]))
2022-04-25 16:07:56,820 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.layer_norm.weight: 6.20803, (torch.Size([512]))
2022-04-25 16:07:56,820 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_2.bias: 5.83009, (torch.Size([512]))
2022-04-25 16:07:56,820 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.w_qs.weight: 5.75765, (torch.Size([512, 1024]))
2022-04-25 16:07:56,820 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.w_ks.weight: 5.52579, (torch.Size([512, 1024]))
2022-04-25 16:07:56,820 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.layer_norm.bias: 5.45425, (torch.Size([512]))
2022-04-25 16:07:56,820 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.layer_norm.weight: 5.04828, (torch.Size([512]))
2022-04-25 16:07:56,820 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.layer_norm.bias: 4.89014, (torch.Size([512]))
2022-04-25 16:07:56,821 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.layer_norm.bias: 4.43194, (torch.Size([512]))
2022-04-25 16:07:56,821 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.layer_norm.bias: 4.21436, (torch.Size([512]))
2022-04-25 16:07:56,821 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.layer_norm.weight: 4.17662, (torch.Size([512]))
2022-04-25 16:07:56,821 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.layer_norm.bias: 3.93753, (torch.Size([512]))
2022-04-25 16:07:56,821 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_1.bias: 3.91729, (torch.Size([2048]))
2022-04-25 16:07:56,821 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: 3.91320, (torch.Size([128]))
2022-04-25 16:07:56,821 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.layer_norm.weight: 3.80293, (torch.Size([512]))
2022-04-25 16:07:56,821 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.layer_norm.bias: 3.73653, (torch.Size([512]))
2022-04-25 16:07:56,821 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.obj_embed1.weight: 3.37730, (torch.Size([151, 200]))
2022-04-25 16:07:56,822 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.layer_norm.weight: 3.35556, (torch.Size([512]))
2022-04-25 16:07:56,822 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.layer_norm.weight: 3.09924, (torch.Size([512]))
2022-04-25 16:07:56,822 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.fc.bias: 3.03677, (torch.Size([512]))
2022-04-25 16:07:56,822 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_vs.bias: 2.89363, (torch.Size([512]))
2022-04-25 16:07:56,822 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.layer_norm.weight: 2.66280, (torch.Size([512]))
2022-04-25 16:07:56,822 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_1.bias: 2.64381, (torch.Size([2048]))
2022-04-25 16:07:56,822 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: 2.58895, (torch.Size([32, 9]))
2022-04-25 16:07:56,822 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.layer_norm.weight: 2.58275, (torch.Size([512]))
2022-04-25 16:07:56,822 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_1.bias: 1.97347, (torch.Size([2048]))
2022-04-25 16:07:56,823 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.rect_conv.0.weight: 1.96007, (torch.Size([128, 2, 7, 7]))
2022-04-25 16:07:56,823 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: 1.87338, (torch.Size([32]))
2022-04-25 16:07:56,823 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.fc.bias: 1.64125, (torch.Size([512]))
2022-04-25 16:07:56,823 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.mix_ctx.bias  : 1.59092, (torch.Size([1024]))
2022-04-25 16:07:56,823 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_vs.bias: 1.57798, (torch.Size([512]))
2022-04-25 16:07:56,823 maskrcnn_benchmark INFO: module.roi_heads.relation.box_feature_extractor.fc6.bias: 1.57483, (torch.Size([4096]))
2022-04-25 16:07:56,823 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.layer_norm.bias: 1.55734, (torch.Size([1024]))
2022-04-25 16:07:56,823 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.pos_ffn.layer_norm.bias: 1.54831, (torch.Size([1024]))
2022-04-25 16:07:56,823 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.layer_norm.bias: 1.53563, (torch.Size([1024]))
2022-04-25 16:07:56,824 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.pos_ffn.layer_norm.bias: 1.53122, (torch.Size([1024]))
2022-04-25 16:07:56,824 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_vs.bias: 1.23652, (torch.Size([512]))
2022-04-25 16:07:56,824 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.fc.bias: 1.19010, (torch.Size([512]))
2022-04-25 16:07:56,824 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.pos_ffn.layer_norm.weight: 1.18717, (torch.Size([1024]))
2022-04-25 16:07:56,824 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.pos_ffn.layer_norm.weight: 1.15254, (torch.Size([1024]))
2022-04-25 16:07:56,824 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.rel_visual_clf.bias: 1.10317, (torch.Size([51]))
2022-04-25 16:07:56,824 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.rel_clf.bias  : 1.10317, (torch.Size([51]))
2022-04-25 16:07:56,824 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.layer_norm.weight: 1.06173, (torch.Size([1024]))
2022-04-25 16:07:56,824 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.layer_norm.weight: 1.00536, (torch.Size([1024]))
2022-04-25 16:07:56,825 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_qs.bias: 0.99065, (torch.Size([512]))
2022-04-25 16:07:56,825 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.pos_ffn.w_2.bias: 0.98012, (torch.Size([1024]))
2022-04-25 16:07:56,825 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.pos_ffn.w_2.bias: 0.92343, (torch.Size([1024]))
2022-04-25 16:07:56,825 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.out_obj.bias: 0.85393, (torch.Size([151]))
2022-04-25 16:07:56,825 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: 0.81558, (torch.Size([4096]))
2022-04-25 16:07:56,825 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.pred_up_dim.bias: 0.64981, (torch.Size([4096]))
2022-04-25 16:07:56,825 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 0.50895, (torch.Size([256]))
2022-04-25 16:07:56,825 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.rect_conv.0.bias: 0.46100, (torch.Size([128]))
2022-04-25 16:07:56,825 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_qs.bias: 0.40766, (torch.Size([512]))
2022-04-25 16:07:56,826 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.post_obj_edge_repr.bias: 0.37753, (torch.Size([1024]))
2022-04-25 16:07:56,826 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 0.32837, (torch.Size([4096]))
2022-04-25 16:07:56,826 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.pos_ffn.w_1.bias: 0.32281, (torch.Size([1024]))
2022-04-25 16:07:56,826 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_qs.bias: 0.29996, (torch.Size([512]))
2022-04-25 16:07:56,826 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.pos_ffn.w_1.bias: 0.28755, (torch.Size([1024]))
2022-04-25 16:07:56,826 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 0.27464, (torch.Size([256]))
2022-04-25 16:07:56,826 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.post_rel2ctx.bias: 0.25710, (torch.Size([4096]))
2022-04-25 16:07:56,826 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_qs.bias: 0.25208, (torch.Size([512]))
2022-04-25 16:07:56,826 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.fc.bias: 0.19209, (torch.Size([1024]))
2022-04-25 16:07:56,827 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.fc.bias: 0.18570, (torch.Size([1024]))
2022-04-25 16:07:56,827 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.bias_module.bias_module.obj_baseline.weight: 0.16569, (torch.Size([22801, 51]))
2022-04-25 16:07:56,827 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.w_vs.bias: 0.15777, (torch.Size([512]))
2022-04-25 16:07:56,827 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.w_vs.bias: 0.15045, (torch.Size([512]))
2022-04-25 16:07:56,827 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 0.12239, (torch.Size([256]))
2022-04-25 16:07:56,827 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 0.10756, (torch.Size([128]))
2022-04-25 16:07:56,827 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 0.10341, (torch.Size([256]))
2022-04-25 16:07:56,827 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 0.10039, (torch.Size([128]))
2022-04-25 16:07:56,827 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.w_qs.bias: 0.03890, (torch.Size([512]))
2022-04-25 16:07:56,828 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.w_qs.bias: 0.03054, (torch.Size([512]))
2022-04-25 16:07:56,828 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2022-04-25 16:07:56,828 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2022-04-25 16:07:56,828 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2022-04-25 16:07:56,828 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2022-04-25 16:07:56,828 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2022-04-25 16:07:56,828 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2022-04-25 16:07:56,828 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.obj_embed2.weight: 0.00000, (torch.Size([151, 200]))
2022-04-25 16:07:56,828 maskrcnn_benchmark INFO: -------------------------------
2022-04-25 16:20:09,487 maskrcnn_benchmark INFO: eta: 18:19:13  iter: 200  loss: 1.9317 (3.3933)  loss_refine_obj: 1.0637 (1.3621)  loss_rel: 0.8584 (2.0312)  time: 3.6775 (3.7052)  data: 0.3397 (0.3416)  lr: 0.007331  max mem: 10498
2022-04-25 16:32:19,207 maskrcnn_benchmark INFO: eta: 17:58:33  iter: 400  loss: 1.5775 (2.5577)  loss_refine_obj: 0.8190 (1.1499)  loss_rel: 0.8451 (1.4078)  time: 3.6607 (3.6769)  data: 0.3458 (0.3473)  lr: 0.013091  max mem: 10498
2022-04-25 16:44:40,622 maskrcnn_benchmark INFO: eta: 17:49:13  iter: 600  loss: 1.1800 (2.1831)  loss_refine_obj: 0.6675 (1.0166)  loss_rel: 0.4586 (1.1664)  time: 3.7519 (3.6870)  data: 0.3154 (0.3487)  lr: 0.016000  max mem: 10498
2022-04-25 16:57:11,592 maskrcnn_benchmark INFO: eta: 17:41:47  iter: 800  loss: 1.0397 (1.9304)  loss_refine_obj: 0.5958 (0.9240)  loss_rel: 0.4668 (1.0063)  time: 3.6867 (3.7039)  data: 0.3339 (0.3495)  lr: 0.016000  max mem: 10525
2022-04-25 17:09:38,364 maskrcnn_benchmark INFO: eta: 17:31:08  iter: 1000  loss: 0.9438 (1.7547)  loss_refine_obj: 0.5892 (0.8601)  loss_rel: 0.3644 (0.8946)  time: 3.7160 (3.7099)  data: 0.3378 (0.3510)  lr: 0.016000  max mem: 10525
2022-04-25 17:22:05,863 maskrcnn_benchmark INFO: eta: 17:20:03  iter: 1200  loss: 0.9465 (1.6280)  loss_refine_obj: 0.5831 (0.8155)  loss_rel: 0.3302 (0.8125)  time: 3.7338 (3.7145)  data: 0.3417 (0.3511)  lr: 0.016000  max mem: 10525
2022-04-25 17:34:33,141 maskrcnn_benchmark INFO: eta: 17:08:32  iter: 1400  loss: 0.9240 (1.5315)  loss_refine_obj: 0.5489 (0.7811)  loss_rel: 0.3653 (0.7504)  time: 3.6774 (3.7176)  data: 0.3569 (0.3524)  lr: 0.016000  max mem: 10525
2022-04-25 17:46:58,528 maskrcnn_benchmark INFO: eta: 16:56:28  iter: 1600  loss: 0.8589 (1.4559)  loss_refine_obj: 0.5718 (0.7545)  loss_rel: 0.3077 (0.7014)  time: 3.6683 (3.7188)  data: 0.3348 (0.3523)  lr: 0.016000  max mem: 10525
2022-04-25 18:00:24,599 maskrcnn_benchmark INFO: eta: 16:53:25  iter: 1800  loss: 0.8821 (1.3913)  loss_refine_obj: 0.5480 (0.7325)  loss_rel: 0.3109 (0.6588)  time: 4.5227 (3.7534)  data: 0.3306 (0.3565)  lr: 0.016000  max mem: 10525
2022-04-25 18:16:37,299 maskrcnn_benchmark INFO: eta: 17:10:30  iter: 2000  loss: 0.8415 (1.3398)  loss_refine_obj: 0.5446 (0.7149)  loss_rel: 0.2779 (0.6250)  time: 4.6419 (3.8644)  data: 0.3501 (0.3738)  lr: 0.016000  max mem: 10525
2022-04-25 18:16:37,299 maskrcnn_benchmark INFO: Start validating
2022-04-25 18:16:37,424 maskrcnn_benchmark INFO: Start evaluation on VG_stanford_filtered_with_attribute_val dataset(5000 images).
2022-04-25 19:11:32,971 maskrcnn_benchmark INFO: Total run time: 0:54:55.545989 (1.318218395614624 s / img per device, on 2 devices)
2022-04-25 19:11:32,971 maskrcnn_benchmark INFO: Model inference time: 0:45:03.793603 (1.081517441368103 s / img per device, on 2 devices)
2022-04-25 19:20:43,582 maskrcnn_benchmark INFO: 
====================================================================================================
Detection evaluation mAp=0.2779
====================================================================================================
SGG eval:     R @ 20: 0.1055;     R @ 50: 0.1483;     R @ 100: 0.1815;  for mode=sgdet, type=Recall(Main).
SGG eval:  ng-R @ 20: 0.1183;  ng-R @ 50: 0.1757;  ng-R @ 100: 0.2287;  for mode=sgdet, type=No Graph Constraint Recall(Main).
SGG eval:    zR @ 20: 0.0000;    zR @ 50: 0.0000;    zR @ 100: 0.0000;  for mode=sgdet, type=Zero Shot Recall.
SGG eval: ng-zR @ 20: 0.0000; ng-zR @ 50: 0.0000; ng-zR @ 100: 0.0000;  for mode=sgdet, type=No Graph Constraint Zero Shot Recall.
SGG eval:    mR @ 20: 0.0530;    mR @ 50: 0.0712;    mR @ 100: 0.0833;  for mode=sgdet, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0143) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.4125) (attached to:0.0000) (behind:0.0206) (belonging to:0.0000) (between:0.0000) (carrying:0.3816) (covered in:0.0000) (covering:0.0000) (eating:0.0000) (flying in:0.0000) (for:0.0370) (from:0.0000) (growing on:0.0000) (hanging from:0.0000) (has:0.3880) (holding:0.1485) (in:0.1031) (in front of:0.1371) (laying on:0.1429) (looking at:0.1087) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.0730) (of:0.1242) (on:0.1399) (on back of:0.0000) (over:0.0122) (painted on:0.0000) (parked on:0.1539) (part of:0.0000) (playing:0.0000) (riding:0.5491) (says:0.0000) (sitting on:0.1670) (standing on:0.2536) (to:0.0000) (under:0.1416) (using:0.0000) (walking in:0.0000) (walking on:0.0270) (watching:0.0196) (wearing:0.6064) (wears:0.0000) (with:0.0009) 
--------------------------------------------------------
SGG eval: ng-mR @ 20: 0.0577; ng-mR @ 50: 0.0829; ng-mR @ 100: 0.1054;  for mode=sgdet, type=No Graph Constraint Mean Recall.
----------------------- Details ------------------------
(above:0.0722) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.4125) (attached to:0.0000) (behind:0.0140) (belonging to:0.0429) (between:0.0000) (carrying:0.3816) (covered in:0.0000) (covering:0.0000) (eating:0.0000) (flying in:0.0000) (for:0.0370) (from:0.0000) (growing on:0.0000) (hanging from:0.0000) (has:0.3943) (holding:0.2152) (in:0.1269) (in front of:0.1341) (laying on:0.1667) (looking at:0.1087) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.0972) (of:0.1791) (on:0.2020) (on back of:0.0000) (over:0.0366) (painted on:0.0000) (parked on:0.4847) (part of:0.0000) (playing:0.0000) (riding:0.5670) (says:0.0000) (sitting on:0.2147) (standing on:0.2899) (to:0.0000) (under:0.1747) (using:0.0000) (walking in:0.0000) (walking on:0.1954) (watching:0.0196) (wearing:0.6212) (wears:0.0701) (with:0.0129) 
--------------------------------------------------------
====================================================================================================

2022-04-25 19:20:45,489 maskrcnn_benchmark INFO: Validation Result: 0.1815
2022-04-25 19:37:04,349 maskrcnn_benchmark INFO: eta: 1 day, 1:02:54  iter: 2200  loss: 0.8233 (1.2951)  loss_refine_obj: 0.5443 (0.7007)  loss_rel: 0.2837 (0.5944)  time: 4.6320 (5.7072)  data: 0.3603 (2.1370)  lr: 0.016000  max mem: 10525
2022-04-25 19:53:23,137 maskrcnn_benchmark INFO: eta: 1 day, 0:26:15  iter: 2400  loss: 0.7993 (1.2562)  loss_refine_obj: 0.5508 (0.6875)  loss_rel: 0.2481 (0.5688)  time: 4.7228 (5.6395)  data: 0.3494 (2.0039)  lr: 0.016000  max mem: 10525
2022-04-25 20:09:30,838 maskrcnn_benchmark INFO: eta: 23:51:38  iter: 2600  loss: 0.7922 (1.2222)  loss_refine_obj: 0.5330 (0.6762)  loss_rel: 0.2821 (0.5461)  time: 4.6727 (5.5778)  data: 0.3618 (1.8888)  lr: 0.016000  max mem: 10525
2022-04-25 20:25:27,402 maskrcnn_benchmark INFO: eta: 23:18:40  iter: 2800  loss: 0.7870 (1.1916)  loss_refine_obj: 0.5114 (0.6665)  loss_rel: 0.2421 (0.5251)  time: 4.5918 (5.5211)  data: 0.3446 (1.7859)  lr: 0.016000  max mem: 10525
2022-04-25 20:41:24,090 maskrcnn_benchmark INFO: eta: 22:47:58  iter: 3000  loss: 0.8122 (1.1656)  loss_refine_obj: 0.5502 (0.6581)  loss_rel: 0.2638 (0.5076)  time: 4.5030 (5.4719)  data: 0.3329 (1.6961)  lr: 0.016000  max mem: 10525
2022-04-25 20:57:40,316 maskrcnn_benchmark INFO: eta: 22:20:37  iter: 3200  loss: 0.7686 (1.1416)  loss_refine_obj: 0.5306 (0.6504)  loss_rel: 0.2281 (0.4912)  time: 4.5779 (5.4350)  data: 0.3569 (1.6233)  lr: 0.016000  max mem: 10525
2022-04-25 21:13:55,491 maskrcnn_benchmark INFO: eta: 21:54:30  iter: 3400  loss: 0.7454 (1.1194)  loss_refine_obj: 0.5202 (0.6435)  loss_rel: 0.2078 (0.4759)  time: 4.5986 (5.4021)  data: 0.3703 (1.5578)  lr: 0.016000  max mem: 10525
2022-04-25 21:30:04,588 maskrcnn_benchmark INFO: eta: 21:29:04  iter: 3600  loss: 0.7588 (1.0990)  loss_refine_obj: 0.5165 (0.6367)  loss_rel: 0.2163 (0.4623)  time: 4.6411 (5.3712)  data: 0.3626 (1.4991)  lr: 0.016000  max mem: 10525
2022-04-25 21:46:17,648 maskrcnn_benchmark INFO: eta: 21:04:52  iter: 3800  loss: 0.7007 (1.0795)  loss_refine_obj: 0.5006 (0.6305)  loss_rel: 0.2000 (0.4490)  time: 4.6096 (5.3445)  data: 0.3740 (1.4467)  lr: 0.016000  max mem: 10525
2022-04-25 22:02:21,081 maskrcnn_benchmark INFO: ---Total norm 1.79844 clip coef 2.78018-----------------
2022-04-25 22:02:21,138 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.rel_clf.weight: 0.85379, (torch.Size([51, 1024]))
2022-04-25 22:02:21,138 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.mix_ctx.weight: 0.76750, (torch.Size([1024, 8192]))
2022-04-25 22:02:21,138 maskrcnn_benchmark INFO: module.roi_heads.relation.box_feature_extractor.fc6.weight: 0.59222, (torch.Size([4096, 12544]))
2022-04-25 22:02:21,138 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.out_obj.weight: 0.58055, (torch.Size([151, 512]))
2022-04-25 22:02:21,139 maskrcnn_benchmark INFO: module.roi_heads.relation.box_feature_extractor.fc7.weight: 0.51558, (torch.Size([4096, 4096]))
2022-04-25 22:02:21,139 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.pred_up_dim.weight: 0.41683, (torch.Size([4096, 1024]))
2022-04-25 22:02:21,139 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.lin_obj.weight: 0.40274, (torch.Size([512, 4424]))
2022-04-25 22:02:21,139 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_2.weight: 0.30995, (torch.Size([512, 2048, 1]))
2022-04-25 22:02:21,139 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_1.weight: 0.28368, (torch.Size([2048, 512, 1]))
2022-04-25 22:02:21,139 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: 0.22516, (torch.Size([4096, 12544]))
2022-04-25 22:02:21,139 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_2.weight: 0.19589, (torch.Size([512, 2048, 1]))
2022-04-25 22:02:21,139 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_2.weight: 0.19130, (torch.Size([512, 2048, 1]))
2022-04-25 22:02:21,140 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_2.weight: 0.18722, (torch.Size([512, 2048, 1]))
2022-04-25 22:02:21,140 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.fc.weight: 0.17044, (torch.Size([512, 512]))
2022-04-25 22:02:21,140 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.fc.weight: 0.16731, (torch.Size([512, 512]))
2022-04-25 22:02:21,140 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.fc.weight: 0.16567, (torch.Size([512, 512]))
2022-04-25 22:02:21,140 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.fc.weight: 0.15826, (torch.Size([512, 512]))
2022-04-25 22:02:21,140 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: 0.12854, (torch.Size([4096, 4096]))
2022-04-25 22:02:21,140 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.rel_visual_clf.weight: 0.10906, (torch.Size([51, 4096]))
2022-04-25 22:02:21,140 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.pos_ffn.w_1.weight: 0.09751, (torch.Size([1024, 1024, 1]))
2022-04-25 22:02:21,141 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.pos_ffn.w_2.weight: 0.09632, (torch.Size([1024, 1024, 1]))
2022-04-25 22:02:21,141 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.w_qs.weight: 0.09396, (torch.Size([512, 1024]))
2022-04-25 22:02:21,141 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: 0.09390, (torch.Size([256, 1024, 3, 3]))
2022-04-25 22:02:21,141 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.pos_ffn.w_2.weight: 0.09241, (torch.Size([1024, 1024, 1]))
2022-04-25 22:02:21,141 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.pos_ffn.w_1.weight: 0.09124, (torch.Size([1024, 1024, 1]))
2022-04-25 22:02:21,141 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_1.weight: 0.09064, (torch.Size([2048, 512, 1]))
2022-04-25 22:02:21,141 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.w_ks.weight: 0.08918, (torch.Size([512, 1024]))
2022-04-25 22:02:21,141 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.post_rel2ctx.weight: 0.08803, (torch.Size([4096, 1024]))
2022-04-25 22:02:21,142 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_1.weight: 0.08436, (torch.Size([2048, 512, 1]))
2022-04-25 22:02:21,142 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.rect_conv.4.weight: 0.07711, (torch.Size([256, 128, 3, 3]))
2022-04-25 22:02:21,142 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.w_qs.weight: 0.07399, (torch.Size([512, 1024]))
2022-04-25 22:02:21,142 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.w_ks.weight: 0.07067, (torch.Size([512, 1024]))
2022-04-25 22:02:21,142 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_1.weight: 0.06982, (torch.Size([2048, 512, 1]))
2022-04-25 22:02:21,142 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_vs.weight: 0.06769, (torch.Size([512, 512]))
2022-04-25 22:02:21,142 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_vs.weight: 0.06730, (torch.Size([512, 512]))
2022-04-25 22:02:21,143 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_vs.weight: 0.06565, (torch.Size([512, 512]))
2022-04-25 22:02:21,143 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_vs.weight: 0.06378, (torch.Size([512, 512]))
2022-04-25 22:02:21,143 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.fc.weight: 0.05544, (torch.Size([1024, 512]))
2022-04-25 22:02:21,143 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.fc.weight: 0.04788, (torch.Size([1024, 512]))
2022-04-25 22:02:21,143 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.rect_conv.0.weight: 0.04654, (torch.Size([128, 2, 7, 7]))
2022-04-25 22:02:21,143 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.w_vs.weight: 0.04074, (torch.Size([512, 1024]))
2022-04-25 22:02:21,143 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.out_obj.bias: 0.04019, (torch.Size([151]))
2022-04-25 22:02:21,143 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.layer_norm.bias: 0.03799, (torch.Size([512]))
2022-04-25 22:02:21,144 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.w_vs.weight: 0.03359, (torch.Size([512, 1024]))
2022-04-25 22:02:21,144 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.post_obj_edge_repr.weight: 0.03299, (torch.Size([1024, 512]))
2022-04-25 22:02:21,144 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.layer_norm.bias: 0.02878, (torch.Size([512]))
2022-04-25 22:02:21,144 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_qs.weight: 0.02873, (torch.Size([512, 512]))
2022-04-25 22:02:21,144 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.layer_norm.bias: 0.02849, (torch.Size([512]))
2022-04-25 22:02:21,144 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.layer_norm.bias: 0.02779, (torch.Size([512]))
2022-04-25 22:02:21,144 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.layer_norm.bias: 0.02754, (torch.Size([512]))
2022-04-25 22:02:21,144 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.layer_norm.bias: 0.02721, (torch.Size([512]))
2022-04-25 22:02:21,145 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.lin_obj.bias: 0.02706, (torch.Size([512]))
2022-04-25 22:02:21,145 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.layer_norm.bias: 0.02703, (torch.Size([512]))
2022-04-25 22:02:21,145 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.layer_norm.bias: 0.02678, (torch.Size([512]))
2022-04-25 22:02:21,145 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_ks.weight: 0.02663, (torch.Size([512, 512]))
2022-04-25 22:02:21,145 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_qs.weight: 0.02459, (torch.Size([512, 512]))
2022-04-25 22:02:21,145 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_ks.weight: 0.02251, (torch.Size([512, 512]))
2022-04-25 22:02:21,145 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.layer_norm.weight: 0.02159, (torch.Size([512]))
2022-04-25 22:02:21,145 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.layer_norm.weight: 0.02111, (torch.Size([512]))
2022-04-25 22:02:21,146 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.layer_norm.weight: 0.02080, (torch.Size([512]))
2022-04-25 22:02:21,146 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.layer_norm.weight: 0.02005, (torch.Size([512]))
2022-04-25 22:02:21,146 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.layer_norm.weight: 0.01983, (torch.Size([512]))
2022-04-25 22:02:21,146 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.layer_norm.weight: 0.01961, (torch.Size([512]))
2022-04-25 22:02:21,146 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.layer_norm.weight: 0.01961, (torch.Size([512]))
2022-04-25 22:02:21,146 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.layer_norm.weight: 0.01869, (torch.Size([512]))
2022-04-25 22:02:21,146 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_qs.weight: 0.01799, (torch.Size([512, 512]))
2022-04-25 22:02:21,146 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_2.bias: 0.01684, (torch.Size([512]))
2022-04-25 22:02:21,147 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_ks.weight: 0.01657, (torch.Size([512, 512]))
2022-04-25 22:02:21,147 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_qs.weight: 0.01346, (torch.Size([512, 512]))
2022-04-25 22:02:21,147 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_ks.weight: 0.01320, (torch.Size([512, 512]))
2022-04-25 22:02:21,147 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_1.bias: 0.01145, (torch.Size([2048]))
2022-04-25 22:02:21,147 maskrcnn_benchmark INFO: module.roi_heads.relation.box_feature_extractor.fc7.bias: 0.01070, (torch.Size([4096]))
2022-04-25 22:02:21,147 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_2.bias: 0.00904, (torch.Size([512]))
2022-04-25 22:02:21,147 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.rect_conv.0.bias: 0.00831, (torch.Size([128]))
2022-04-25 22:02:21,147 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.bias_module.bias_module.obj_baseline.weight: 0.00719, (torch.Size([22801, 51]))
2022-04-25 22:02:21,148 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.rel_visual_clf.bias: 0.00654, (torch.Size([51]))
2022-04-25 22:02:21,148 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.rel_clf.bias  : 0.00654, (torch.Size([51]))
2022-04-25 22:02:21,148 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.obj_embed1.weight: 0.00541, (torch.Size([151, 200]))
2022-04-25 22:02:21,148 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: 0.00521, (torch.Size([128, 32]))
2022-04-25 22:02:21,148 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_2.bias: 0.00496, (torch.Size([512]))
2022-04-25 22:02:21,148 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_2.bias: 0.00479, (torch.Size([512]))
2022-04-25 22:02:21,148 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.fc.bias: 0.00461, (torch.Size([512]))
2022-04-25 22:02:21,148 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: 0.00451, (torch.Size([4096]))
2022-04-25 22:02:21,148 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_vs.bias: 0.00432, (torch.Size([512]))
2022-04-25 22:02:21,149 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_1.bias: 0.00361, (torch.Size([2048]))
2022-04-25 22:02:21,149 maskrcnn_benchmark INFO: module.roi_heads.relation.box_feature_extractor.fc6.bias: 0.00358, (torch.Size([4096]))
2022-04-25 22:02:21,149 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: 0.00286, (torch.Size([128]))
2022-04-25 22:02:21,149 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.pos_ffn.layer_norm.bias: 0.00250, (torch.Size([1024]))
2022-04-25 22:02:21,149 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.mix_ctx.bias  : 0.00249, (torch.Size([1024]))
2022-04-25 22:02:21,149 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.pos_ffn.layer_norm.weight: 0.00227, (torch.Size([1024]))
2022-04-25 22:02:21,149 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.pos_ffn.layer_norm.weight: 0.00225, (torch.Size([1024]))
2022-04-25 22:02:21,149 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.pos_ffn.layer_norm.bias: 0.00223, (torch.Size([1024]))
2022-04-25 22:02:21,150 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 0.00222, (torch.Size([256]))
2022-04-25 22:02:21,150 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.layer_norm.weight: 0.00220, (torch.Size([1024]))
2022-04-25 22:02:21,150 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.layer_norm.bias: 0.00219, (torch.Size([1024]))
2022-04-25 22:02:21,150 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.pos_ffn.w_2.bias: 0.00216, (torch.Size([1024]))
2022-04-25 22:02:21,150 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: 0.00216, (torch.Size([32, 9]))
2022-04-25 22:02:21,150 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_1.bias: 0.00215, (torch.Size([2048]))
2022-04-25 22:02:21,150 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.fc.bias: 0.00213, (torch.Size([512]))
2022-04-25 22:02:21,150 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.fc.bias: 0.00211, (torch.Size([512]))
2022-04-25 22:02:21,151 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.layer_norm.bias: 0.00207, (torch.Size([1024]))
2022-04-25 22:02:21,151 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.pos_ffn.w_2.bias: 0.00205, (torch.Size([1024]))
2022-04-25 22:02:21,151 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_vs.bias: 0.00195, (torch.Size([512]))
2022-04-25 22:02:21,151 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.layer_norm.weight: 0.00194, (torch.Size([1024]))
2022-04-25 22:02:21,151 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_1.bias: 0.00186, (torch.Size([2048]))
2022-04-25 22:02:21,151 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_vs.bias: 0.00180, (torch.Size([512]))
2022-04-25 22:02:21,151 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.fc.bias: 0.00177, (torch.Size([512]))
2022-04-25 22:02:21,151 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 0.00158, (torch.Size([256]))
2022-04-25 22:02:21,152 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 0.00156, (torch.Size([128]))
2022-04-25 22:02:21,152 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_vs.bias: 0.00153, (torch.Size([512]))
2022-04-25 22:02:21,152 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: 0.00147, (torch.Size([32]))
2022-04-25 22:02:21,152 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 0.00138, (torch.Size([4096]))
2022-04-25 22:02:21,152 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 0.00109, (torch.Size([256]))
2022-04-25 22:02:21,152 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.pred_up_dim.bias: 0.00109, (torch.Size([4096]))
2022-04-25 22:02:21,152 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.pos_ffn.w_1.bias: 0.00101, (torch.Size([1024]))
2022-04-25 22:02:21,152 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.post_obj_edge_repr.bias: 0.00096, (torch.Size([1024]))
2022-04-25 22:02:21,153 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.post_rel2ctx.bias: 0.00093, (torch.Size([4096]))
2022-04-25 22:02:21,153 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 0.00085, (torch.Size([256]))
2022-04-25 22:02:21,153 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.pos_ffn.w_1.bias: 0.00083, (torch.Size([1024]))
2022-04-25 22:02:21,153 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_qs.bias: 0.00081, (torch.Size([512]))
2022-04-25 22:02:21,153 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.w_qs.bias: 0.00079, (torch.Size([512]))
2022-04-25 22:02:21,153 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 0.00075, (torch.Size([128]))
2022-04-25 22:02:21,153 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_qs.bias: 0.00074, (torch.Size([512]))
2022-04-25 22:02:21,153 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.w_qs.bias: 0.00067, (torch.Size([512]))
2022-04-25 22:02:21,154 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_qs.bias: 0.00055, (torch.Size([512]))
2022-04-25 22:02:21,154 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_qs.bias: 0.00049, (torch.Size([512]))
2022-04-25 22:02:21,154 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.fc.bias: 0.00041, (torch.Size([1024]))
2022-04-25 22:02:21,154 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.fc.bias: 0.00037, (torch.Size([1024]))
2022-04-25 22:02:21,154 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.w_vs.bias: 0.00032, (torch.Size([512]))
2022-04-25 22:02:21,154 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.w_vs.bias: 0.00029, (torch.Size([512]))
2022-04-25 22:02:21,154 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2022-04-25 22:02:21,154 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2022-04-25 22:02:21,155 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2022-04-25 22:02:21,155 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2022-04-25 22:02:21,155 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2022-04-25 22:02:21,155 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2022-04-25 22:02:21,155 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.obj_embed2.weight: 0.00000, (torch.Size([151, 200]))
2022-04-25 22:02:21,155 maskrcnn_benchmark INFO: -------------------------------
2022-04-25 22:02:21,167 maskrcnn_benchmark INFO: eta: 20:40:54  iter: 4000  loss: 0.7515 (1.0614)  loss_refine_obj: 0.5361 (0.6247)  loss_rel: 0.1845 (0.4367)  time: 4.6341 (5.3182)  data: 0.3324 (1.3973)  lr: 0.016000  max mem: 10525
2022-04-25 22:02:21,450 maskrcnn_benchmark.utils.checkpoint INFO: Saving checkpoint to dual/model_0004000.pth
2022-04-25 22:02:23,701 maskrcnn_benchmark INFO: Start validating
2022-04-25 22:02:23,745 maskrcnn_benchmark INFO: Start evaluation on VG_stanford_filtered_with_attribute_val dataset(5000 images).
2022-04-25 22:56:41,704 maskrcnn_benchmark INFO: Total run time: 0:54:17.959337 (1.3031837347984314 s / img per device, on 2 devices)
2022-04-25 22:56:41,705 maskrcnn_benchmark INFO: Model inference time: 0:38:55.729963 (0.9342919850349426 s / img per device, on 2 devices)
2022-04-25 23:05:57,553 maskrcnn_benchmark INFO: 
====================================================================================================
Detection evaluation mAp=0.2875
====================================================================================================
SGG eval:     R @ 20: 0.1710;     R @ 50: 0.2260;     R @ 100: 0.2642;  for mode=sgdet, type=Recall(Main).
SGG eval:  ng-R @ 20: 0.1965;  ng-R @ 50: 0.2776;  ng-R @ 100: 0.3440;  for mode=sgdet, type=No Graph Constraint Recall(Main).
SGG eval:    zR @ 20: 0.0000;    zR @ 50: 0.0000;    zR @ 100: 0.0000;  for mode=sgdet, type=Zero Shot Recall.
SGG eval: ng-zR @ 20: 0.0000; ng-zR @ 50: 0.0000; ng-zR @ 100: 0.0000;  for mode=sgdet, type=No Graph Constraint Zero Shot Recall.
SGG eval:    mR @ 20: 0.0782;    mR @ 50: 0.1032;    mR @ 100: 0.1219;  for mode=sgdet, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0624) (across:0.0000) (against:0.0263) (along:0.1282) (and:0.0000) (at:0.4656) (attached to:0.0601) (behind:0.1537) (belonging to:0.0000) (between:0.0000) (carrying:0.1228) (covered in:0.0714) (covering:0.0000) (eating:0.3333) (flying in:0.0000) (for:0.1111) (from:0.0000) (growing on:0.0000) (hanging from:0.2390) (has:0.2841) (holding:0.3137) (in:0.2072) (in front of:0.2296) (laying on:0.0952) (looking at:0.1739) (lying on:0.0000) (made of:0.0000) (mounted on:0.0145) (near:0.0985) (of:0.2683) (on:0.2586) (on back of:0.0455) (over:0.0366) (painted on:0.0000) (parked on:0.0246) (part of:0.0000) (playing:0.0000) (riding:0.6384) (says:0.0000) (sitting on:0.1597) (standing on:0.1007) (to:0.0278) (under:0.0672) (using:0.0000) (walking in:0.0000) (walking on:0.4989) (watching:0.0490) (wearing:0.6227) (wears:0.0251) (with:0.0790) 
--------------------------------------------------------
SGG eval: ng-mR @ 20: 0.0947; ng-mR @ 50: 0.1316; ng-mR @ 100: 0.1740;  for mode=sgdet, type=No Graph Constraint Mean Recall.
----------------------- Details ------------------------
(above:0.2291) (across:0.0000) (against:0.0263) (along:0.2051) (and:0.0000) (at:0.4555) (attached to:0.1025) (behind:0.1392) (belonging to:0.0738) (between:0.0000) (carrying:0.3640) (covered in:0.0000) (covering:0.1571) (eating:0.3333) (flying in:0.0000) (for:0.0741) (from:0.0000) (growing on:0.0000) (hanging from:0.3051) (has:0.3142) (holding:0.3467) (in:0.3490) (in front of:0.2705) (laying on:0.2143) (looking at:0.1522) (lying on:0.0000) (made of:0.0000) (mounted on:0.0435) (near:0.1849) (of:0.3093) (on:0.3489) (on back of:0.0909) (over:0.0732) (painted on:0.0000) (parked on:0.0437) (part of:0.1042) (playing:0.0000) (riding:0.6518) (says:0.0000) (sitting on:0.3008) (standing on:0.2529) (to:0.0833) (under:0.1054) (using:0.1538) (walking in:0.0000) (walking on:0.4665) (watching:0.0490) (wearing:0.6378) (wears:0.5025) (with:0.1845) 
--------------------------------------------------------
====================================================================================================

2022-04-25 23:06:00,458 maskrcnn_benchmark INFO: Validation Result: 0.2642
2022-04-25 23:22:07,711 maskrcnn_benchmark INFO: eta: 23:47:03  iter: 4200  loss: 0.6998 (1.0457)  loss_refine_obj: 0.5139 (0.6199)  loss_rel: 0.1949 (0.4259)  time: 4.6216 (6.2046)  data: 0.4188 (2.2636)  lr: 0.016000  max mem: 10525
2022-04-25 23:38:07,500 maskrcnn_benchmark INFO: eta: 23:11:53  iter: 4400  loss: 0.6990 (1.0305)  loss_refine_obj: 0.5239 (0.6150)  loss_rel: 0.1797 (0.4155)  time: 4.4921 (6.1407)  data: 0.3480 (2.1806)  lr: 0.016000  max mem: 10525
2022-04-25 23:54:04,717 maskrcnn_benchmark INFO: eta: 22:38:16  iter: 4600  loss: 0.6722 (1.0161)  loss_refine_obj: 0.4855 (0.6103)  loss_rel: 0.1685 (0.4058)  time: 4.5969 (6.0818)  data: 0.3558 (2.1048)  lr: 0.016000  max mem: 10525
2022-04-26 00:09:55,326 maskrcnn_benchmark INFO: eta: 22:05:48  iter: 4800  loss: 0.6980 (1.0033)  loss_refine_obj: 0.4877 (0.6061)  loss_rel: 0.1830 (0.3972)  time: 4.5460 (6.0264)  data: 0.3625 (2.0356)  lr: 0.016000  max mem: 10525
2022-04-26 00:25:52,622 maskrcnn_benchmark INFO: eta: 21:34:58  iter: 5000  loss: 0.6689 (0.9912)  loss_refine_obj: 0.4892 (0.6023)  loss_rel: 0.1767 (0.3889)  time: 4.7441 (5.9768)  data: 0.3786 (1.9717)  lr: 0.016000  max mem: 10525
2022-04-26 00:42:11,564 maskrcnn_benchmark INFO: eta: 21:06:10  iter: 5200  loss: 0.6832 (0.9802)  loss_refine_obj: 0.5004 (0.5992)  loss_rel: 0.1732 (0.3811)  time: 4.7057 (5.9352)  data: 0.3711 (1.9150)  lr: 0.016000  max mem: 10525
2022-04-26 00:58:21,949 maskrcnn_benchmark INFO: eta: 20:37:58  iter: 5400  loss: 0.6974 (0.9695)  loss_refine_obj: 0.5017 (0.5958)  loss_rel: 0.1829 (0.3737)  time: 4.6114 (5.8951)  data: 0.3549 (1.8628)  lr: 0.016000  max mem: 10525
2022-04-26 01:14:51,266 maskrcnn_benchmark INFO: eta: 20:11:19  iter: 5600  loss: 0.6986 (0.9595)  loss_refine_obj: 0.5118 (0.5929)  loss_rel: 0.1711 (0.3666)  time: 4.6217 (5.8612)  data: 0.3581 (1.8163)  lr: 0.016000  max mem: 10525
2022-04-26 01:30:46,615 maskrcnn_benchmark INFO: eta: 19:44:10  iter: 5800  loss: 0.6701 (0.9501)  loss_refine_obj: 0.5111 (0.5901)  loss_rel: 0.1671 (0.3601)  time: 4.5950 (5.8238)  data: 0.3674 (1.7697)  lr: 0.016000  max mem: 10525
2022-04-26 01:46:57,166 maskrcnn_benchmark INFO: eta: 19:18:17  iter: 6000  loss: 0.6757 (0.9411)  loss_refine_obj: 0.5142 (0.5871)  loss_rel: 0.1585 (0.3540)  time: 4.6402 (5.7915)  data: 0.3457 (1.7284)  lr: 0.016000  max mem: 10525
2022-04-26 01:46:57,166 maskrcnn_benchmark INFO: Start validating
2022-04-26 01:46:57,229 maskrcnn_benchmark INFO: Start evaluation on VG_stanford_filtered_with_attribute_val dataset(5000 images).
2022-04-26 02:40:55,742 maskrcnn_benchmark INFO: Total run time: 0:53:58.511954 (1.2954047815322876 s / img per device, on 2 devices)
2022-04-26 02:40:55,742 maskrcnn_benchmark INFO: Model inference time: 0:38:59.423970 (0.9357695881843567 s / img per device, on 2 devices)
2022-04-26 02:50:13,367 maskrcnn_benchmark INFO: 
====================================================================================================
Detection evaluation mAp=0.2920
====================================================================================================
SGG eval:     R @ 20: 0.2048;     R @ 50: 0.2671;     R @ 100: 0.3112;  for mode=sgdet, type=Recall(Main).
SGG eval:  ng-R @ 20: 0.2239;  ng-R @ 50: 0.3048;  ng-R @ 100: 0.3680;  for mode=sgdet, type=No Graph Constraint Recall(Main).
SGG eval:    zR @ 20: 0.0000;    zR @ 50: 0.0000;    zR @ 100: 0.0000;  for mode=sgdet, type=Zero Shot Recall.
SGG eval: ng-zR @ 20: 0.0000; ng-zR @ 50: 0.0000; ng-zR @ 100: 0.0000;  for mode=sgdet, type=No Graph Constraint Zero Shot Recall.
SGG eval:    mR @ 20: 0.0928;    mR @ 50: 0.1161;    mR @ 100: 0.1362;  for mode=sgdet, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0100) (across:0.0000) (against:0.0526) (along:0.1667) (and:0.0161) (at:0.3737) (attached to:0.0138) (behind:0.1489) (belonging to:0.0333) (between:0.0000) (carrying:0.3904) (covered in:0.0000) (covering:0.0000) (eating:0.4762) (flying in:0.0000) (for:0.0370) (from:0.0000) (growing on:0.0000) (hanging from:0.1618) (has:0.3438) (holding:0.2148) (in:0.1553) (in front of:0.0354) (laying on:0.0476) (looking at:0.0000) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.1668) (of:0.2600) (on:0.3504) (on back of:0.0909) (over:0.0854) (painted on:0.0000) (parked on:0.5981) (part of:0.0000) (playing:0.0000) (riding:0.5967) (says:0.0000) (sitting on:0.1405) (standing on:0.0942) (to:0.3056) (under:0.0804) (using:0.1538) (walking in:0.0000) (walking on:0.4740) (watching:0.0490) (wearing:0.6675) (wears:0.0000) (with:0.0198) 
--------------------------------------------------------
SGG eval: ng-mR @ 20: 0.1061; ng-mR @ 50: 0.1494; ng-mR @ 100: 0.1913;  for mode=sgdet, type=No Graph Constraint Mean Recall.
----------------------- Details ------------------------
(above:0.2001) (across:0.0833) (against:0.0789) (along:0.2051) (and:0.0161) (at:0.3713) (attached to:0.0799) (behind:0.1286) (belonging to:0.1929) (between:0.0000) (carrying:0.4079) (covered in:0.0714) (covering:0.0571) (eating:0.4762) (flying in:0.0000) (for:0.0370) (from:0.0000) (growing on:0.0000) (hanging from:0.3456) (has:0.3253) (holding:0.3199) (in:0.2750) (in front of:0.1772) (laying on:0.1429) (looking at:0.0435) (lying on:0.1111) (made of:0.0000) (mounted on:0.0435) (near:0.2241) (of:0.3837) (on:0.4040) (on back of:0.1818) (over:0.1220) (painted on:0.0000) (parked on:0.6184) (part of:0.0000) (playing:0.0000) (riding:0.5565) (says:0.0000) (sitting on:0.3087) (standing on:0.2083) (to:0.3056) (under:0.1403) (using:0.2308) (walking in:0.0000) (walking on:0.4160) (watching:0.0294) (wearing:0.6579) (wears:0.4545) (with:0.1341) 
--------------------------------------------------------
====================================================================================================

2022-04-26 02:50:15,846 maskrcnn_benchmark INFO: Validation Result: 0.3112
2022-04-26 03:02:36,230 maskrcnn_benchmark INFO: eta: 20:46:13  iter: 6200  loss: 0.6801 (0.9328)  loss_refine_obj: 0.4923 (0.5846)  loss_rel: 0.1510 (0.3482)  time: 3.7425 (6.3367)  data: 0.3230 (2.2962)  lr: 0.016000  max mem: 10525
2022-04-26 03:14:57,451 maskrcnn_benchmark INFO: eta: 20:09:12  iter: 6400  loss: 0.6909 (0.9252)  loss_refine_obj: 0.5011 (0.5822)  loss_rel: 0.1684 (0.3429)  time: 3.6906 (6.2545)  data: 0.3509 (2.2355)  lr: 0.016000  max mem: 10525
2022-04-26 03:27:22,255 maskrcnn_benchmark INFO: eta: 19:33:47  iter: 6600  loss: 0.6451 (0.9176)  loss_refine_obj: 0.4849 (0.5799)  loss_rel: 0.1386 (0.3377)  time: 3.6772 (6.1778)  data: 0.3265 (2.1782)  lr: 0.016000  max mem: 10525
2022-04-26 03:39:40,944 maskrcnn_benchmark INFO: eta: 18:59:33  iter: 6800  loss: 0.6778 (0.9104)  loss_refine_obj: 0.4882 (0.5775)  loss_rel: 0.1865 (0.3329)  time: 3.6762 (6.1048)  data: 0.3103 (2.1241)  lr: 0.016000  max mem: 10525
2022-04-26 03:51:59,429 maskrcnn_benchmark INFO: eta: 18:26:34  iter: 7000  loss: 0.6394 (0.9036)  loss_refine_obj: 0.4682 (0.5754)  loss_rel: 0.1602 (0.3282)  time: 3.6814 (6.0359)  data: 0.3159 (2.0733)  lr: 0.016000  max mem: 10525
2022-04-26 04:04:21,371 maskrcnn_benchmark INFO: eta: 17:54:49  iter: 7200  loss: 0.6515 (0.8971)  loss_refine_obj: 0.5073 (0.5734)  loss_rel: 0.1494 (0.3237)  time: 3.6835 (5.9712)  data: 0.3392 (2.0254)  lr: 0.016000  max mem: 10525
2022-04-26 04:16:38,813 maskrcnn_benchmark INFO: eta: 17:24:00  iter: 7400  loss: 0.6655 (0.8908)  loss_refine_obj: 0.4822 (0.5713)  loss_rel: 0.1662 (0.3195)  time: 3.6242 (5.9095)  data: 0.3299 (1.9799)  lr: 0.016000  max mem: 10525
2022-04-26 04:29:00,087 maskrcnn_benchmark INFO: eta: 16:54:15  iter: 7600  loss: 0.6526 (0.8843)  loss_refine_obj: 0.4614 (0.5689)  loss_rel: 0.1608 (0.3154)  time: 3.6851 (5.8515)  data: 0.3374 (1.9372)  lr: 0.016000  max mem: 10525
2022-04-26 04:41:16,596 maskrcnn_benchmark INFO: eta: 16:25:18  iter: 7800  loss: 0.6295 (0.8784)  loss_refine_obj: 0.4720 (0.5669)  loss_rel: 0.1469 (0.3115)  time: 3.6021 (5.7959)  data: 0.3242 (1.8962)  lr: 0.016000  max mem: 10525
2022-04-26 04:54:30,179 maskrcnn_benchmark INFO: ---Total norm 1.43692 clip coef 3.47966-----------------
2022-04-26 04:54:30,218 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.rel_clf.weight: 0.67559, (torch.Size([51, 1024]))
2022-04-26 04:54:30,218 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.out_obj.weight: 0.52934, (torch.Size([151, 512]))
2022-04-26 04:54:30,218 maskrcnn_benchmark INFO: module.roi_heads.relation.box_feature_extractor.fc6.weight: 0.52873, (torch.Size([4096, 12544]))
2022-04-26 04:54:30,218 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.mix_ctx.weight: 0.52451, (torch.Size([1024, 8192]))
2022-04-26 04:54:30,218 maskrcnn_benchmark INFO: module.roi_heads.relation.box_feature_extractor.fc7.weight: 0.47147, (torch.Size([4096, 4096]))
2022-04-26 04:54:30,219 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.lin_obj.weight: 0.32885, (torch.Size([512, 4424]))
2022-04-26 04:54:30,219 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.pred_up_dim.weight: 0.30575, (torch.Size([4096, 1024]))
2022-04-26 04:54:30,219 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: 0.26492, (torch.Size([4096, 12544]))
2022-04-26 04:54:30,219 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_2.weight: 0.17752, (torch.Size([512, 2048, 1]))
2022-04-26 04:54:30,219 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_1.weight: 0.16368, (torch.Size([2048, 512, 1]))
2022-04-26 04:54:30,219 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: 0.14471, (torch.Size([4096, 4096]))
2022-04-26 04:54:30,219 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_2.weight: 0.13114, (torch.Size([512, 2048, 1]))
2022-04-26 04:54:30,220 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: 0.12462, (torch.Size([256, 1024, 3, 3]))
2022-04-26 04:54:30,220 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.rel_visual_clf.weight: 0.12006, (torch.Size([51, 4096]))
2022-04-26 04:54:30,220 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_2.weight: 0.11352, (torch.Size([512, 2048, 1]))
2022-04-26 04:54:30,220 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_2.weight: 0.10913, (torch.Size([512, 2048, 1]))
2022-04-26 04:54:30,220 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.rect_conv.4.weight: 0.10795, (torch.Size([256, 128, 3, 3]))
2022-04-26 04:54:30,220 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.pos_ffn.w_2.weight: 0.10202, (torch.Size([1024, 1024, 1]))
2022-04-26 04:54:30,220 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.fc.weight: 0.09982, (torch.Size([512, 512]))
2022-04-26 04:54:30,221 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.fc.weight: 0.09825, (torch.Size([512, 512]))
2022-04-26 04:54:30,221 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.fc.weight: 0.09699, (torch.Size([512, 512]))
2022-04-26 04:54:30,221 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.pos_ffn.w_1.weight: 0.09415, (torch.Size([1024, 1024, 1]))
2022-04-26 04:54:30,221 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.fc.weight: 0.09220, (torch.Size([512, 512]))
2022-04-26 04:54:30,221 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.post_rel2ctx.weight: 0.08996, (torch.Size([4096, 1024]))
2022-04-26 04:54:30,221 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.pos_ffn.w_2.weight: 0.08325, (torch.Size([1024, 1024, 1]))
2022-04-26 04:54:30,222 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.pos_ffn.w_1.weight: 0.07624, (torch.Size([1024, 1024, 1]))
2022-04-26 04:54:30,222 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.rect_conv.0.weight: 0.07364, (torch.Size([128, 2, 7, 7]))
2022-04-26 04:54:30,222 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_1.weight: 0.07326, (torch.Size([2048, 512, 1]))
2022-04-26 04:54:30,222 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_1.weight: 0.05134, (torch.Size([2048, 512, 1]))
2022-04-26 04:54:30,222 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.fc.weight: 0.04551, (torch.Size([1024, 512]))
2022-04-26 04:54:30,222 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.fc.weight: 0.04521, (torch.Size([1024, 512]))
2022-04-26 04:54:30,222 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_vs.weight: 0.04436, (torch.Size([512, 512]))
2022-04-26 04:54:30,223 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_1.weight: 0.04330, (torch.Size([2048, 512, 1]))
2022-04-26 04:54:30,223 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_vs.weight: 0.04173, (torch.Size([512, 512]))
2022-04-26 04:54:30,223 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_vs.weight: 0.04015, (torch.Size([512, 512]))
2022-04-26 04:54:30,223 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_vs.weight: 0.03905, (torch.Size([512, 512]))
2022-04-26 04:54:30,223 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.w_ks.weight: 0.03117, (torch.Size([512, 1024]))
2022-04-26 04:54:30,223 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.w_qs.weight: 0.03077, (torch.Size([512, 1024]))
2022-04-26 04:54:30,223 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.w_ks.weight: 0.02994, (torch.Size([512, 1024]))
2022-04-26 04:54:30,224 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.w_qs.weight: 0.02908, (torch.Size([512, 1024]))
2022-04-26 04:54:30,224 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.w_vs.weight: 0.02770, (torch.Size([512, 1024]))
2022-04-26 04:54:30,224 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.w_vs.weight: 0.02662, (torch.Size([512, 1024]))
2022-04-26 04:54:30,224 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.out_obj.bias: 0.02549, (torch.Size([151]))
2022-04-26 04:54:30,224 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.post_obj_edge_repr.weight: 0.02404, (torch.Size([1024, 512]))
2022-04-26 04:54:30,224 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.layer_norm.bias: 0.02018, (torch.Size([512]))
2022-04-26 04:54:30,224 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_qs.weight: 0.01700, (torch.Size([512, 512]))
2022-04-26 04:54:30,225 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_qs.weight: 0.01667, (torch.Size([512, 512]))
2022-04-26 04:54:30,225 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.layer_norm.bias: 0.01633, (torch.Size([512]))
2022-04-26 04:54:30,225 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.layer_norm.bias: 0.01617, (torch.Size([512]))
2022-04-26 04:54:30,225 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.lin_obj.bias: 0.01614, (torch.Size([512]))
2022-04-26 04:54:30,225 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.layer_norm.bias: 0.01594, (torch.Size([512]))
2022-04-26 04:54:30,225 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.layer_norm.bias: 0.01584, (torch.Size([512]))
2022-04-26 04:54:30,225 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.layer_norm.bias: 0.01584, (torch.Size([512]))
2022-04-26 04:54:30,226 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.layer_norm.bias: 0.01574, (torch.Size([512]))
2022-04-26 04:54:30,226 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.layer_norm.bias: 0.01572, (torch.Size([512]))
2022-04-26 04:54:30,226 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_ks.weight: 0.01533, (torch.Size([512, 512]))
2022-04-26 04:54:30,226 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_ks.weight: 0.01490, (torch.Size([512, 512]))
2022-04-26 04:54:30,226 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.layer_norm.weight: 0.01352, (torch.Size([512]))
2022-04-26 04:54:30,226 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.layer_norm.weight: 0.01352, (torch.Size([512]))
2022-04-26 04:54:30,227 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_qs.weight: 0.01329, (torch.Size([512, 512]))
2022-04-26 04:54:30,227 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.layer_norm.weight: 0.01313, (torch.Size([512]))
2022-04-26 04:54:30,227 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_ks.weight: 0.01289, (torch.Size([512, 512]))
2022-04-26 04:54:30,227 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_qs.weight: 0.01262, (torch.Size([512, 512]))
2022-04-26 04:54:30,227 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.layer_norm.weight: 0.01246, (torch.Size([512]))
2022-04-26 04:54:30,227 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_ks.weight: 0.01241, (torch.Size([512, 512]))
2022-04-26 04:54:30,227 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.layer_norm.weight: 0.01218, (torch.Size([512]))
2022-04-26 04:54:30,228 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.layer_norm.weight: 0.01207, (torch.Size([512]))
2022-04-26 04:54:30,228 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.layer_norm.weight: 0.01163, (torch.Size([512]))
2022-04-26 04:54:30,228 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.layer_norm.weight: 0.01132, (torch.Size([512]))
2022-04-26 04:54:30,228 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.bias_module.bias_module.obj_baseline.weight: 0.01057, (torch.Size([22801, 51]))
2022-04-26 04:54:30,228 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.rel_visual_clf.bias: 0.00962, (torch.Size([51]))
2022-04-26 04:54:30,228 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.rel_clf.bias  : 0.00962, (torch.Size([51]))
2022-04-26 04:54:30,229 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_2.bias: 0.00908, (torch.Size([512]))
2022-04-26 04:54:30,229 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.rect_conv.0.bias: 0.00838, (torch.Size([128]))
2022-04-26 04:54:30,229 maskrcnn_benchmark INFO: module.roi_heads.relation.box_feature_extractor.fc7.bias: 0.00762, (torch.Size([4096]))
2022-04-26 04:54:30,229 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_2.bias: 0.00669, (torch.Size([512]))
2022-04-26 04:54:30,229 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: 0.00540, (torch.Size([4096]))
2022-04-26 04:54:30,229 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_1.bias: 0.00532, (torch.Size([2048]))
2022-04-26 04:54:30,229 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.obj_embed1.weight: 0.00523, (torch.Size([151, 200]))
2022-04-26 04:54:30,230 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 0.00503, (torch.Size([256]))
2022-04-26 04:54:30,230 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 0.00412, (torch.Size([128]))
2022-04-26 04:54:30,230 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.pos_ffn.w_2.bias: 0.00383, (torch.Size([1024]))
2022-04-26 04:54:30,230 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: 0.00337, (torch.Size([128, 32]))
2022-04-26 04:54:30,230 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_2.bias: 0.00328, (torch.Size([512]))
2022-04-26 04:54:30,230 maskrcnn_benchmark INFO: module.roi_heads.relation.box_feature_extractor.fc6.bias: 0.00314, (torch.Size([4096]))
2022-04-26 04:54:30,230 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.pos_ffn.layer_norm.bias: 0.00306, (torch.Size([1024]))
2022-04-26 04:54:30,231 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_2.bias: 0.00301, (torch.Size([512]))
2022-04-26 04:54:30,231 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.pos_ffn.w_2.bias: 0.00296, (torch.Size([1024]))
2022-04-26 04:54:30,231 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.fc.bias: 0.00284, (torch.Size([512]))
2022-04-26 04:54:30,231 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.pos_ffn.layer_norm.weight: 0.00276, (torch.Size([1024]))
2022-04-26 04:54:30,231 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_1.bias: 0.00267, (torch.Size([2048]))
2022-04-26 04:54:30,231 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 0.00252, (torch.Size([256]))
2022-04-26 04:54:30,231 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_vs.bias: 0.00246, (torch.Size([512]))
2022-04-26 04:54:30,232 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.pos_ffn.layer_norm.weight: 0.00237, (torch.Size([1024]))
2022-04-26 04:54:30,232 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.mix_ctx.bias  : 0.00235, (torch.Size([1024]))
2022-04-26 04:54:30,232 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.pos_ffn.layer_norm.bias: 0.00232, (torch.Size([1024]))
2022-04-26 04:54:30,232 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.layer_norm.bias: 0.00230, (torch.Size([1024]))
2022-04-26 04:54:30,232 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.layer_norm.bias: 0.00226, (torch.Size([1024]))
2022-04-26 04:54:30,232 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.layer_norm.weight: 0.00226, (torch.Size([1024]))
2022-04-26 04:54:30,232 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.layer_norm.weight: 0.00225, (torch.Size([1024]))
2022-04-26 04:54:30,233 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.post_rel2ctx.bias: 0.00210, (torch.Size([4096]))
2022-04-26 04:54:30,233 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: 0.00187, (torch.Size([128]))
2022-04-26 04:54:30,233 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 0.00187, (torch.Size([4096]))
2022-04-26 04:54:30,233 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.pos_ffn.w_1.bias: 0.00171, (torch.Size([1024]))
2022-04-26 04:54:30,233 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 0.00165, (torch.Size([256]))
2022-04-26 04:54:30,233 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 0.00154, (torch.Size([256]))
2022-04-26 04:54:30,234 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 0.00151, (torch.Size([128]))
2022-04-26 04:54:30,234 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.fc.bias: 0.00135, (torch.Size([512]))
2022-04-26 04:54:30,234 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.fc.bias: 0.00134, (torch.Size([512]))
2022-04-26 04:54:30,234 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: 0.00127, (torch.Size([32, 9]))
2022-04-26 04:54:30,234 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_1.bias: 0.00124, (torch.Size([2048]))
2022-04-26 04:54:30,234 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_vs.bias: 0.00123, (torch.Size([512]))
2022-04-26 04:54:30,234 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.fc.bias: 0.00116, (torch.Size([512]))
2022-04-26 04:54:30,235 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_vs.bias: 0.00114, (torch.Size([512]))
2022-04-26 04:54:30,235 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_1.bias: 0.00111, (torch.Size([2048]))
2022-04-26 04:54:30,235 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.pos_ffn.w_1.bias: 0.00110, (torch.Size([1024]))
2022-04-26 04:54:30,235 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.pred_up_dim.bias: 0.00099, (torch.Size([4096]))
2022-04-26 04:54:30,235 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_vs.bias: 0.00099, (torch.Size([512]))
2022-04-26 04:54:30,235 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: 0.00084, (torch.Size([32]))
2022-04-26 04:54:30,235 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.post_obj_edge_repr.bias: 0.00082, (torch.Size([1024]))
2022-04-26 04:54:30,236 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_qs.bias: 0.00059, (torch.Size([512]))
2022-04-26 04:54:30,236 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.fc.bias: 0.00049, (torch.Size([1024]))
2022-04-26 04:54:30,236 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.fc.bias: 0.00048, (torch.Size([1024]))
2022-04-26 04:54:30,236 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_qs.bias: 0.00044, (torch.Size([512]))
2022-04-26 04:54:30,236 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.w_qs.bias: 0.00043, (torch.Size([512]))
2022-04-26 04:54:30,236 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.w_qs.bias: 0.00041, (torch.Size([512]))
2022-04-26 04:54:30,236 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_qs.bias: 0.00039, (torch.Size([512]))
2022-04-26 04:54:30,236 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_qs.bias: 0.00037, (torch.Size([512]))
2022-04-26 04:54:30,237 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.w_vs.bias: 0.00036, (torch.Size([512]))
2022-04-26 04:54:30,237 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.w_vs.bias: 0.00035, (torch.Size([512]))
2022-04-26 04:54:30,237 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2022-04-26 04:54:30,237 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2022-04-26 04:54:30,237 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2022-04-26 04:54:30,237 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2022-04-26 04:54:30,237 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2022-04-26 04:54:30,238 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2022-04-26 04:54:30,238 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.obj_embed2.weight: 0.00000, (torch.Size([151, 200]))
2022-04-26 04:54:30,238 maskrcnn_benchmark INFO: -------------------------------
2022-04-26 04:54:30,251 maskrcnn_benchmark INFO: eta: 15:58:22  iter: 8000  loss: 0.6327 (0.8727)  loss_refine_obj: 0.4571 (0.5649)  loss_rel: 0.1797 (0.3079)  time: 4.6113 (5.7502)  data: 0.3528 (1.8582)  lr: 0.016000  max mem: 10525
2022-04-26 04:54:30,498 maskrcnn_benchmark.utils.checkpoint INFO: Saving checkpoint to dual/model_0008000.pth
2022-04-26 04:54:33,224 maskrcnn_benchmark INFO: Start validating
2022-04-26 04:54:34,005 maskrcnn_benchmark INFO: Start evaluation on VG_stanford_filtered_with_attribute_val dataset(5000 images).
2022-04-26 05:48:44,854 maskrcnn_benchmark INFO: Total run time: 0:54:10.848739 (1.300339495563507 s / img per device, on 2 devices)
2022-04-26 05:48:44,854 maskrcnn_benchmark INFO: Model inference time: 0:39:06.723800 (0.9386895198822022 s / img per device, on 2 devices)
2022-04-26 05:58:01,554 maskrcnn_benchmark INFO: 
====================================================================================================
Detection evaluation mAp=0.2894
====================================================================================================
SGG eval:     R @ 20: 0.2201;     R @ 50: 0.2876;     R @ 100: 0.3332;  for mode=sgdet, type=Recall(Main).
SGG eval:  ng-R @ 20: 0.2345;  ng-R @ 50: 0.3215;  ng-R @ 100: 0.3883;  for mode=sgdet, type=No Graph Constraint Recall(Main).
SGG eval:    zR @ 20: 0.0000;    zR @ 50: 0.0000;    zR @ 100: 0.0000;  for mode=sgdet, type=Zero Shot Recall.
SGG eval: ng-zR @ 20: 0.0000; ng-zR @ 50: 0.0000; ng-zR @ 100: 0.0044;  for mode=sgdet, type=No Graph Constraint Zero Shot Recall.
SGG eval:    mR @ 20: 0.0759;    mR @ 50: 0.1013;    mR @ 100: 0.1195;  for mode=sgdet, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0334) (across:0.0000) (against:0.0263) (along:0.0385) (and:0.0161) (at:0.2298) (attached to:0.0000) (behind:0.1871) (belonging to:0.0000) (between:0.0000) (carrying:0.4342) (covered in:0.2857) (covering:0.0000) (eating:0.3333) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0294) (has:0.3646) (holding:0.2214) (in:0.1602) (in front of:0.0369) (laying on:0.0000) (looking at:0.0000) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.1808) (of:0.1884) (on:0.4089) (on back of:0.0000) (over:0.0610) (painted on:0.0000) (parked on:0.5621) (part of:0.0000) (playing:0.0000) (riding:0.4836) (says:0.0000) (sitting on:0.1300) (standing on:0.0236) (to:0.1944) (under:0.0548) (using:0.0769) (walking in:0.0000) (walking on:0.4952) (watching:0.0588) (wearing:0.6508) (wears:0.0000) (with:0.0061) 
--------------------------------------------------------
SGG eval: ng-mR @ 20: 0.0903; ng-mR @ 50: 0.1320; ng-mR @ 100: 0.1765;  for mode=sgdet, type=No Graph Constraint Mean Recall.
----------------------- Details ------------------------
(above:0.2251) (across:0.0556) (against:0.0789) (along:0.0833) (and:0.0323) (at:0.3582) (attached to:0.0197) (behind:0.1916) (belonging to:0.0690) (between:0.0000) (carrying:0.4605) (covered in:0.2857) (covering:0.0000) (eating:0.4762) (flying in:0.0000) (for:0.0370) (from:0.0000) (growing on:0.0000) (hanging from:0.2022) (has:0.3608) (holding:0.3308) (in:0.3037) (in front of:0.1673) (laying on:0.0952) (looking at:0.0435) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.2281) (of:0.3411) (on:0.4503) (on back of:0.1364) (over:0.1037) (painted on:0.0000) (parked on:0.5873) (part of:0.0816) (playing:0.0000) (riding:0.4747) (says:0.0000) (sitting on:0.2313) (standing on:0.1304) (to:0.3056) (under:0.1182) (using:0.2308) (walking in:0.0000) (walking on:0.4470) (watching:0.0588) (wearing:0.6291) (wears:0.2494) (with:0.1449) 
--------------------------------------------------------
====================================================================================================

2022-04-26 05:58:03,491 maskrcnn_benchmark INFO: Validation Result: 0.3332
2022-04-26 06:14:17,193 maskrcnn_benchmark INFO: eta: 16:51:38  iter: 8200  loss: 0.6670 (0.8674)  loss_refine_obj: 0.5065 (0.5631)  loss_rel: 0.1519 (0.3043)  time: 4.6574 (6.1937)  data: 0.3641 (2.2912)  lr: 0.016000  max mem: 10525
2022-04-26 06:30:40,714 maskrcnn_benchmark INFO: eta: 16:26:08  iter: 8400  loss: 0.6573 (0.8623)  loss_refine_obj: 0.4785 (0.5614)  loss_rel: 0.1614 (0.3009)  time: 4.6419 (6.1634)  data: 0.3814 (2.2495)  lr: 0.016000  max mem: 10525
2022-04-26 06:46:46,196 maskrcnn_benchmark INFO: eta: 16:00:43  iter: 8600  loss: 0.6424 (0.8573)  loss_refine_obj: 0.5004 (0.5596)  loss_rel: 0.1453 (0.2976)  time: 4.5737 (6.1323)  data: 0.3800 (2.2090)  lr: 0.016000  max mem: 10525
2022-04-26 07:02:57,896 maskrcnn_benchmark INFO: eta: 15:35:50  iter: 8800  loss: 0.6488 (0.8525)  loss_refine_obj: 0.4871 (0.5581)  loss_rel: 0.1587 (0.2945)  time: 4.6607 (6.1033)  data: 0.3364 (2.1687)  lr: 0.016000  max mem: 10525
2022-04-26 07:19:09,382 maskrcnn_benchmark INFO: eta: 15:11:20  iter: 9000  loss: 0.6311 (0.8481)  loss_refine_obj: 0.4675 (0.5566)  loss_rel: 0.1537 (0.2915)  time: 4.7784 (6.0757)  data: 0.3395 (2.1304)  lr: 0.016000  max mem: 10525
2022-04-26 07:32:33,954 maskrcnn_benchmark INFO: eta: 14:44:33  iter: 9200  loss: 0.6429 (0.8439)  loss_refine_obj: 0.4771 (0.5551)  loss_rel: 0.1718 (0.2887)  time: 3.6544 (6.0310)  data: 0.3236 (2.0927)  lr: 0.016000  max mem: 10525
2022-04-26 07:44:56,004 maskrcnn_benchmark INFO: eta: 14:17:22  iter: 9400  loss: 0.6374 (0.8398)  loss_refine_obj: 0.4751 (0.5538)  loss_rel: 0.1447 (0.2860)  time: 3.6847 (5.9817)  data: 0.3481 (2.0555)  lr: 0.016000  max mem: 10525
2022-04-26 07:57:23,272 maskrcnn_benchmark INFO: eta: 13:50:52  iter: 9600  loss: 0.6523 (0.8359)  loss_refine_obj: 0.5017 (0.5525)  loss_rel: 0.1325 (0.2833)  time: 3.7962 (5.9349)  data: 0.3521 (2.0200)  lr: 0.016000  max mem: 10525
2022-04-26 08:09:44,482 maskrcnn_benchmark INFO: eta: 13:24:53  iter: 9800  loss: 0.6226 (0.8320)  loss_refine_obj: 0.4629 (0.5512)  loss_rel: 0.1532 (0.2808)  time: 3.6804 (5.8894)  data: 0.3399 (1.9859)  lr: 0.016000  max mem: 10525
2022-04-26 08:22:07,043 maskrcnn_benchmark INFO: eta: 12:59:26  iter: 10000  loss: 0.6632 (0.8284)  loss_refine_obj: 0.5219 (0.5501)  loss_rel: 0.1442 (0.2784)  time: 3.7634 (5.8459)  data: 0.3344 (1.9531)  lr: 0.016000  max mem: 10525
2022-04-26 08:22:07,043 maskrcnn_benchmark INFO: Start validating
2022-04-26 08:22:07,126 maskrcnn_benchmark INFO: Start evaluation on VG_stanford_filtered_with_attribute_val dataset(5000 images).
2022-04-26 09:16:41,801 maskrcnn_benchmark INFO: Total run time: 0:54:34.674639 (1.3098698554992676 s / img per device, on 2 devices)
2022-04-26 09:16:41,801 maskrcnn_benchmark INFO: Model inference time: 0:44:42.909689 (1.0731638756752013 s / img per device, on 2 devices)
2022-04-26 09:25:55,613 maskrcnn_benchmark INFO: 
====================================================================================================
Detection evaluation mAp=0.2948
====================================================================================================
SGG eval:     R @ 20: 0.2301;     R @ 50: 0.2999;     R @ 100: 0.3452;  for mode=sgdet, type=Recall(Main).
SGG eval:  ng-R @ 20: 0.2398;  ng-R @ 50: 0.3268;  ng-R @ 100: 0.3922;  for mode=sgdet, type=No Graph Constraint Recall(Main).
SGG eval:    zR @ 20: 0.0000;    zR @ 50: 0.0000;    zR @ 100: 0.0000;  for mode=sgdet, type=Zero Shot Recall.
SGG eval: ng-zR @ 20: 0.0000; ng-zR @ 50: 0.0000; ng-zR @ 100: 0.0000;  for mode=sgdet, type=No Graph Constraint Zero Shot Recall.
SGG eval:    mR @ 20: 0.0688;    mR @ 50: 0.0944;    mR @ 100: 0.1105;  for mode=sgdet, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0301) (across:0.0000) (against:0.0000) (along:0.1859) (and:0.0000) (at:0.1260) (attached to:0.0000) (behind:0.1437) (belonging to:0.0000) (between:0.0000) (carrying:0.1272) (covered in:0.2857) (covering:0.0000) (eating:0.4762) (flying in:0.0000) (for:0.0370) (from:0.0000) (growing on:0.0000) (hanging from:0.1287) (has:0.4264) (holding:0.3283) (in:0.1598) (in front of:0.0206) (laying on:0.0000) (looking at:0.0435) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.2300) (of:0.1661) (on:0.4244) (on back of:0.0000) (over:0.0366) (painted on:0.0000) (parked on:0.0153) (part of:0.0000) (playing:0.0000) (riding:0.5967) (says:0.0000) (sitting on:0.1658) (standing on:0.0587) (to:0.0694) (under:0.0459) (using:0.0000) (walking in:0.0000) (walking on:0.4769) (watching:0.0588) (wearing:0.6525) (wears:0.0000) (with:0.0086) 
--------------------------------------------------------
SGG eval: ng-mR @ 20: 0.0912; ng-mR @ 50: 0.1422; ng-mR @ 100: 0.1847;  for mode=sgdet, type=No Graph Constraint Mean Recall.
----------------------- Details ------------------------
(above:0.2483) (across:0.0556) (against:0.0526) (along:0.3397) (and:0.0323) (at:0.3873) (attached to:0.0151) (behind:0.1541) (belonging to:0.0214) (between:0.0000) (carrying:0.3816) (covered in:0.2857) (covering:0.0714) (eating:0.4762) (flying in:0.0000) (for:0.0741) (from:0.0000) (growing on:0.0000) (hanging from:0.2243) (has:0.4432) (holding:0.3507) (in:0.3173) (in front of:0.1303) (laying on:0.0952) (looking at:0.0435) (lying on:0.1111) (made of:0.0000) (mounted on:0.0000) (near:0.2733) (of:0.2926) (on:0.4344) (on back of:0.0455) (over:0.0610) (painted on:0.0000) (parked on:0.5115) (part of:0.0000) (playing:0.0000) (riding:0.5967) (says:0.0000) (sitting on:0.3126) (standing on:0.2109) (to:0.2222) (under:0.1097) (using:0.2308) (walking in:0.0000) (walking on:0.4282) (watching:0.0588) (wearing:0.6412) (wears:0.3637) (with:0.1299) 
--------------------------------------------------------
====================================================================================================

2022-04-26 09:25:57,540 maskrcnn_benchmark INFO: Validation Result: 0.3452
2022-04-26 09:38:19,870 maskrcnn_benchmark INFO: eta: 13:23:20  iter: 10200  loss: 0.6316 (0.8247)  loss_refine_obj: 0.4521 (0.5487)  loss_rel: 0.1449 (0.2760)  time: 3.6677 (6.1796)  data: 0.3232 (2.2971)  lr: 0.001600  max mem: 10525
2022-04-26 09:50:39,488 maskrcnn_benchmark INFO: eta: 12:56:41  iter: 10400  loss: 0.6264 (0.8210)  loss_refine_obj: 0.4575 (0.5473)  loss_rel: 0.1527 (0.2737)  time: 3.7502 (6.1318)  data: 0.3304 (2.2594)  lr: 0.001600  max mem: 10525
2022-04-26 10:03:04,394 maskrcnn_benchmark INFO: eta: 12:30:39  iter: 10600  loss: 0.6239 (0.8174)  loss_refine_obj: 0.4772 (0.5460)  loss_rel: 0.1446 (0.2714)  time: 3.8002 (6.0864)  data: 0.3551 (2.2234)  lr: 0.001600  max mem: 10525
2022-04-26 10:15:13,937 maskrcnn_benchmark INFO: eta: 12:04:56  iter: 10800  loss: 0.6181 (0.8139)  loss_refine_obj: 0.4706 (0.5446)  loss_rel: 0.1330 (0.2692)  time: 3.5897 (6.0412)  data: 0.3255 (2.1888)  lr: 0.001600  max mem: 10525
2022-04-26 10:29:16,287 maskrcnn_benchmark INFO: eta: 11:40:55  iter: 11000  loss: 0.5793 (0.8102)  loss_refine_obj: 0.4304 (0.5431)  loss_rel: 0.1271 (0.2671)  time: 4.5374 (6.0080)  data: 0.3709 (2.1563)  lr: 0.001600  max mem: 10525
2022-04-26 10:41:59,113 maskrcnn_benchmark INFO: eta: 11:16:27  iter: 11200  loss: 0.5738 (0.8065)  loss_refine_obj: 0.4394 (0.5416)  loss_rel: 0.1354 (0.2649)  time: 3.6478 (5.9688)  data: 0.3527 (2.1247)  lr: 0.001600  max mem: 10525
2022-04-26 10:54:20,617 maskrcnn_benchmark INFO: eta: 10:52:12  iter: 11400  loss: 0.6082 (0.8030)  loss_refine_obj: 0.4593 (0.5402)  loss_rel: 0.1528 (0.2629)  time: 3.6762 (5.9291)  data: 0.3152 (2.0935)  lr: 0.001600  max mem: 10525
2022-04-26 11:06:41,557 maskrcnn_benchmark INFO: eta: 10:28:21  iter: 11600  loss: 0.6237 (0.7996)  loss_refine_obj: 0.4666 (0.5388)  loss_rel: 0.1495 (0.2609)  time: 3.7005 (5.8908)  data: 0.3484 (2.0634)  lr: 0.001600  max mem: 10525
2022-04-26 11:18:59,424 maskrcnn_benchmark INFO: eta: 10:04:51  iter: 11800  loss: 0.6225 (0.7964)  loss_refine_obj: 0.4580 (0.5374)  loss_rel: 0.1541 (0.2590)  time: 3.7005 (5.8535)  data: 0.3296 (2.0342)  lr: 0.001600  max mem: 10525
2022-04-26 11:31:17,831 maskrcnn_benchmark INFO: ---Total norm 1.17278 clip coef 4.26337-----------------
2022-04-26 11:31:17,880 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.out_obj.weight: 0.49029, (torch.Size([151, 512]))
2022-04-26 11:31:17,881 maskrcnn_benchmark INFO: module.roi_heads.relation.box_feature_extractor.fc6.weight: 0.48094, (torch.Size([4096, 12544]))
2022-04-26 11:31:17,881 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.rel_clf.weight: 0.45427, (torch.Size([51, 1024]))
2022-04-26 11:31:17,881 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.mix_ctx.weight: 0.43745, (torch.Size([1024, 8192]))
2022-04-26 11:31:17,881 maskrcnn_benchmark INFO: module.roi_heads.relation.box_feature_extractor.fc7.weight: 0.41964, (torch.Size([4096, 4096]))
2022-04-26 11:31:17,881 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.lin_obj.weight: 0.28763, (torch.Size([512, 4424]))
2022-04-26 11:31:17,882 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.pred_up_dim.weight: 0.23150, (torch.Size([4096, 1024]))
2022-04-26 11:31:17,882 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: 0.17019, (torch.Size([4096, 12544]))
2022-04-26 11:31:17,882 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_1.weight: 0.14324, (torch.Size([2048, 512, 1]))
2022-04-26 11:31:17,882 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_2.weight: 0.13586, (torch.Size([512, 2048, 1]))
2022-04-26 11:31:17,882 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.pos_ffn.w_2.weight: 0.10767, (torch.Size([1024, 1024, 1]))
2022-04-26 11:31:17,882 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_2.weight: 0.10734, (torch.Size([512, 2048, 1]))
2022-04-26 11:31:17,882 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.pos_ffn.w_2.weight: 0.09060, (torch.Size([1024, 1024, 1]))
2022-04-26 11:31:17,882 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.pos_ffn.w_1.weight: 0.09055, (torch.Size([1024, 1024, 1]))
2022-04-26 11:31:17,883 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_2.weight: 0.08665, (torch.Size([512, 2048, 1]))
2022-04-26 11:31:17,883 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: 0.08406, (torch.Size([4096, 4096]))
2022-04-26 11:31:17,883 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_2.weight: 0.08174, (torch.Size([512, 2048, 1]))
2022-04-26 11:31:17,883 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: 0.07783, (torch.Size([256, 1024, 3, 3]))
2022-04-26 11:31:17,883 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.fc.weight: 0.07528, (torch.Size([512, 512]))
2022-04-26 11:31:17,883 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.fc.weight: 0.06899, (torch.Size([512, 512]))
2022-04-26 11:31:17,883 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.fc.weight: 0.06898, (torch.Size([512, 512]))
2022-04-26 11:31:17,883 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.fc.weight: 0.06860, (torch.Size([512, 512]))
2022-04-26 11:31:17,884 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.pos_ffn.w_1.weight: 0.06699, (torch.Size([1024, 1024, 1]))
2022-04-26 11:31:17,884 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_1.weight: 0.06691, (torch.Size([2048, 512, 1]))
2022-04-26 11:31:17,884 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.rel_visual_clf.weight: 0.06454, (torch.Size([51, 4096]))
2022-04-26 11:31:17,884 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.rect_conv.4.weight: 0.05546, (torch.Size([256, 128, 3, 3]))
2022-04-26 11:31:17,884 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.fc.weight: 0.05172, (torch.Size([1024, 512]))
2022-04-26 11:31:17,884 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.fc.weight: 0.05080, (torch.Size([1024, 512]))
2022-04-26 11:31:17,884 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.post_rel2ctx.weight: 0.04372, (torch.Size([4096, 1024]))
2022-04-26 11:31:17,884 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_1.weight: 0.04200, (torch.Size([2048, 512, 1]))
2022-04-26 11:31:17,885 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_vs.weight: 0.03753, (torch.Size([512, 512]))
2022-04-26 11:31:17,885 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_1.weight: 0.03456, (torch.Size([2048, 512, 1]))
2022-04-26 11:31:17,885 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_vs.weight: 0.03335, (torch.Size([512, 512]))
2022-04-26 11:31:17,885 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.rect_conv.0.weight: 0.03332, (torch.Size([128, 2, 7, 7]))
2022-04-26 11:31:17,885 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_vs.weight: 0.02959, (torch.Size([512, 512]))
2022-04-26 11:31:17,885 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_vs.weight: 0.02949, (torch.Size([512, 512]))
2022-04-26 11:31:17,885 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.w_vs.weight: 0.02671, (torch.Size([512, 1024]))
2022-04-26 11:31:17,885 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.w_vs.weight: 0.02647, (torch.Size([512, 1024]))
2022-04-26 11:31:17,885 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.w_ks.weight: 0.02039, (torch.Size([512, 1024]))
2022-04-26 11:31:17,886 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.w_qs.weight: 0.02000, (torch.Size([512, 1024]))
2022-04-26 11:31:17,886 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.w_ks.weight: 0.01971, (torch.Size([512, 1024]))
2022-04-26 11:31:17,886 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.w_qs.weight: 0.01785, (torch.Size([512, 1024]))
2022-04-26 11:31:17,886 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.post_obj_edge_repr.weight: 0.01723, (torch.Size([1024, 512]))
2022-04-26 11:31:17,886 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_qs.weight: 0.01607, (torch.Size([512, 512]))
2022-04-26 11:31:17,886 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.out_obj.bias: 0.01558, (torch.Size([151]))
2022-04-26 11:31:17,886 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_qs.weight: 0.01510, (torch.Size([512, 512]))
2022-04-26 11:31:17,886 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_ks.weight: 0.01397, (torch.Size([512, 512]))
2022-04-26 11:31:17,887 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_ks.weight: 0.01371, (torch.Size([512, 512]))
2022-04-26 11:31:17,887 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_qs.weight: 0.01342, (torch.Size([512, 512]))
2022-04-26 11:31:17,887 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_ks.weight: 0.01328, (torch.Size([512, 512]))
2022-04-26 11:31:17,887 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_qs.weight: 0.01296, (torch.Size([512, 512]))
2022-04-26 11:31:17,887 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.layer_norm.bias: 0.01256, (torch.Size([512]))
2022-04-26 11:31:17,887 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.rel_visual_clf.bias: 0.01203, (torch.Size([51]))
2022-04-26 11:31:17,887 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.rel_clf.bias  : 0.01203, (torch.Size([51]))
2022-04-26 11:31:17,887 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_ks.weight: 0.01174, (torch.Size([512, 512]))
2022-04-26 11:31:17,887 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.lin_obj.bias: 0.01050, (torch.Size([512]))
2022-04-26 11:31:17,888 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.layer_norm.bias: 0.01031, (torch.Size([512]))
2022-04-26 11:31:17,888 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.layer_norm.bias: 0.01026, (torch.Size([512]))
2022-04-26 11:31:17,888 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.layer_norm.bias: 0.01019, (torch.Size([512]))
2022-04-26 11:31:17,888 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.layer_norm.bias: 0.01018, (torch.Size([512]))
2022-04-26 11:31:17,888 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.layer_norm.bias: 0.01016, (torch.Size([512]))
2022-04-26 11:31:17,888 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.layer_norm.bias: 0.01015, (torch.Size([512]))
2022-04-26 11:31:17,888 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.layer_norm.bias: 0.01015, (torch.Size([512]))
2022-04-26 11:31:17,888 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.layer_norm.weight: 0.00862, (torch.Size([512]))
2022-04-26 11:31:17,889 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.layer_norm.weight: 0.00801, (torch.Size([512]))
2022-04-26 11:31:17,889 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.layer_norm.weight: 0.00787, (torch.Size([512]))
2022-04-26 11:31:17,889 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.bias_module.bias_module.obj_baseline.weight: 0.00784, (torch.Size([22801, 51]))
2022-04-26 11:31:17,889 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.layer_norm.weight: 0.00780, (torch.Size([512]))
2022-04-26 11:31:17,889 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.layer_norm.weight: 0.00774, (torch.Size([512]))
2022-04-26 11:31:17,889 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.layer_norm.weight: 0.00758, (torch.Size([512]))
2022-04-26 11:31:17,889 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.layer_norm.weight: 0.00738, (torch.Size([512]))
2022-04-26 11:31:17,889 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.layer_norm.weight: 0.00719, (torch.Size([512]))
2022-04-26 11:31:17,890 maskrcnn_benchmark INFO: module.roi_heads.relation.box_feature_extractor.fc7.bias: 0.00666, (torch.Size([4096]))
2022-04-26 11:31:17,890 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_2.bias: 0.00625, (torch.Size([512]))
2022-04-26 11:31:17,890 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_2.bias: 0.00567, (torch.Size([512]))
2022-04-26 11:31:17,890 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.pos_ffn.w_2.bias: 0.00495, (torch.Size([1024]))
2022-04-26 11:31:17,890 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.obj_embed1.weight: 0.00485, (torch.Size([151, 200]))
2022-04-26 11:31:17,890 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.rect_conv.0.bias: 0.00464, (torch.Size([128]))
2022-04-26 11:31:17,890 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_1.bias: 0.00416, (torch.Size([2048]))
2022-04-26 11:31:17,890 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.pos_ffn.w_2.bias: 0.00384, (torch.Size([1024]))
2022-04-26 11:31:17,891 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.pos_ffn.layer_norm.bias: 0.00353, (torch.Size([1024]))
2022-04-26 11:31:17,891 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: 0.00319, (torch.Size([4096]))
2022-04-26 11:31:17,891 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.pos_ffn.layer_norm.weight: 0.00313, (torch.Size([1024]))
2022-04-26 11:31:17,891 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.pos_ffn.layer_norm.bias: 0.00296, (torch.Size([1024]))
2022-04-26 11:31:17,891 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.layer_norm.bias: 0.00292, (torch.Size([1024]))
2022-04-26 11:31:17,891 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.pos_ffn.layer_norm.weight: 0.00285, (torch.Size([1024]))
2022-04-26 11:31:17,891 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.mix_ctx.bias  : 0.00284, (torch.Size([1024]))
2022-04-26 11:31:17,891 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.layer_norm.bias: 0.00280, (torch.Size([1024]))
2022-04-26 11:31:17,892 maskrcnn_benchmark INFO: module.roi_heads.relation.box_feature_extractor.fc6.bias: 0.00269, (torch.Size([4096]))
2022-04-26 11:31:17,892 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.layer_norm.weight: 0.00260, (torch.Size([1024]))
2022-04-26 11:31:17,892 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.layer_norm.weight: 0.00260, (torch.Size([1024]))
2022-04-26 11:31:17,892 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_2.bias: 0.00259, (torch.Size([512]))
2022-04-26 11:31:17,892 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: 0.00240, (torch.Size([128, 32]))
2022-04-26 11:31:17,892 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_2.bias: 0.00233, (torch.Size([512]))
2022-04-26 11:31:17,892 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_1.bias: 0.00223, (torch.Size([2048]))
2022-04-26 11:31:17,892 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.fc.bias: 0.00206, (torch.Size([512]))
2022-04-26 11:31:17,892 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.pos_ffn.w_1.bias: 0.00185, (torch.Size([1024]))
2022-04-26 11:31:17,893 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_vs.bias: 0.00170, (torch.Size([512]))
2022-04-26 11:31:17,893 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 0.00162, (torch.Size([128]))
2022-04-26 11:31:17,893 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 0.00153, (torch.Size([256]))
2022-04-26 11:31:17,893 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 0.00134, (torch.Size([256]))
2022-04-26 11:31:17,893 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: 0.00134, (torch.Size([128]))
2022-04-26 11:31:17,893 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.pos_ffn.w_1.bias: 0.00120, (torch.Size([1024]))
2022-04-26 11:31:17,893 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 0.00112, (torch.Size([4096]))
2022-04-26 11:31:17,893 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.fc.bias: 0.00106, (torch.Size([512]))
2022-04-26 11:31:17,894 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.fc.bias: 0.00097, (torch.Size([512]))
2022-04-26 11:31:17,894 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_vs.bias: 0.00095, (torch.Size([512]))
2022-04-26 11:31:17,894 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_1.bias: 0.00094, (torch.Size([2048]))
2022-04-26 11:31:17,894 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.pred_up_dim.bias: 0.00092, (torch.Size([4096]))
2022-04-26 11:31:17,894 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 0.00087, (torch.Size([256]))
2022-04-26 11:31:17,894 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.fc.bias: 0.00085, (torch.Size([512]))
2022-04-26 11:31:17,894 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_1.bias: 0.00084, (torch.Size([2048]))
2022-04-26 11:31:17,894 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.post_rel2ctx.bias: 0.00083, (torch.Size([4096]))
2022-04-26 11:31:17,895 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_vs.bias: 0.00081, (torch.Size([512]))
2022-04-26 11:31:17,895 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.fc.bias: 0.00076, (torch.Size([1024]))
2022-04-26 11:31:17,895 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_vs.bias: 0.00070, (torch.Size([512]))
2022-04-26 11:31:17,895 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.fc.bias: 0.00069, (torch.Size([1024]))
2022-04-26 11:31:17,895 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_qs.bias: 0.00068, (torch.Size([512]))
2022-04-26 11:31:17,895 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 0.00066, (torch.Size([256]))
2022-04-26 11:31:17,895 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.post_obj_edge_repr.bias: 0.00065, (torch.Size([1024]))
2022-04-26 11:31:17,895 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: 0.00062, (torch.Size([32, 9]))
2022-04-26 11:31:17,896 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.w_vs.bias: 0.00048, (torch.Size([512]))
2022-04-26 11:31:17,896 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.w_vs.bias: 0.00047, (torch.Size([512]))
2022-04-26 11:31:17,896 maskrcnn_benchmark INFO: module.roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 0.00045, (torch.Size([128]))
2022-04-26 11:31:17,896 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_qs.bias: 0.00045, (torch.Size([512]))
2022-04-26 11:31:17,896 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: 0.00043, (torch.Size([32]))
2022-04-26 11:31:17,896 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_qs.bias: 0.00038, (torch.Size([512]))
2022-04-26 11:31:17,896 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_qs.bias: 0.00036, (torch.Size([512]))
2022-04-26 11:31:17,896 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.w_qs.bias: 0.00031, (torch.Size([512]))
2022-04-26 11:31:17,896 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.w_qs.bias: 0.00029, (torch.Size([512]))
2022-04-26 11:31:17,897 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.1.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2022-04-26 11:31:17,897 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.graph_encoder.0.graph_encoder.layer_stack.0.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2022-04-26 11:31:17,897 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2022-04-26 11:31:17,897 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2022-04-26 11:31:17,897 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2022-04-26 11:31:17,897 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2022-04-26 11:31:17,897 maskrcnn_benchmark INFO: module.roi_heads.relation.predictor.context_layer.obj_embed2.weight: 0.00000, (torch.Size([151, 200]))
2022-04-26 11:31:17,897 maskrcnn_benchmark INFO: -------------------------------
2022-04-26 11:31:17,907 maskrcnn_benchmark INFO: eta: 9:41:44  iter: 12000  loss: 0.5815 (0.7932)  loss_refine_obj: 0.4426 (0.5362)  loss_rel: 0.1372 (0.2570)  time: 3.7128 (5.8175)  data: 0.3670 (2.0061)  lr: 0.001600  max mem: 10525
2022-04-26 11:31:18,148 maskrcnn_benchmark.utils.checkpoint INFO: Saving checkpoint to dual/model_0012000.pth
2022-04-26 11:31:20,724 maskrcnn_benchmark INFO: Start validating
2022-04-26 11:31:20,795 maskrcnn_benchmark INFO: Start evaluation on VG_stanford_filtered_with_attribute_val dataset(5000 images).
2022-04-26 12:25:36,190 maskrcnn_benchmark INFO: Total run time: 0:54:15.393843 (1.3021575372695924 s / img per device, on 2 devices)
2022-04-26 12:25:36,190 maskrcnn_benchmark INFO: Model inference time: 0:41:51.825350 (1.0047301400184632 s / img per device, on 2 devices)
2022-04-26 12:34:58,379 maskrcnn_benchmark INFO: 
====================================================================================================
Detection evaluation mAp=0.2966
====================================================================================================
SGG eval:     R @ 20: 0.2438;     R @ 50: 0.3114;     R @ 100: 0.3554;  for mode=sgdet, type=Recall(Main).
SGG eval:  ng-R @ 20: 0.2543;  ng-R @ 50: 0.3425;  ng-R @ 100: 0.4057;  for mode=sgdet, type=No Graph Constraint Recall(Main).
SGG eval:    zR @ 20: 0.0000;    zR @ 50: 0.0000;    zR @ 100: 0.0000;  for mode=sgdet, type=Zero Shot Recall.
SGG eval: ng-zR @ 20: 0.0000; ng-zR @ 50: 0.0000; ng-zR @ 100: 0.0000;  for mode=sgdet, type=No Graph Constraint Zero Shot Recall.
SGG eval:    mR @ 20: 0.0738;    mR @ 50: 0.0978;    mR @ 100: 0.1140;  for mode=sgdet, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0301) (across:0.0000) (against:0.0789) (along:0.1282) (and:0.0000) (at:0.2459) (attached to:0.0000) (behind:0.1838) (belonging to:0.0000) (between:0.0000) (carrying:0.3947) (covered in:0.2500) (covering:0.0000) (eating:0.4762) (flying in:0.0000) (for:0.0370) (from:0.0000) (growing on:0.0000) (hanging from:0.1029) (has:0.4254) (holding:0.2604) (in:0.1561) (in front of:0.1069) (laying on:0.0000) (looking at:0.0000) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.1654) (of:0.2434) (on:0.4530) (on back of:0.0000) (over:0.0488) (painted on:0.0000) (parked on:0.0409) (part of:0.0000) (playing:0.0000) (riding:0.4807) (says:0.0000) (sitting on:0.1527) (standing on:0.0145) (to:0.0694) (under:0.0574) (using:0.0000) (walking in:0.0000) (walking on:0.3748) (watching:0.0588) (wearing:0.6463) (wears:0.0000) (with:0.0159) 
--------------------------------------------------------
SGG eval: ng-mR @ 20: 0.0972; ng-mR @ 50: 0.1456; ng-mR @ 100: 0.1938;  for mode=sgdet, type=No Graph Constraint Mean Recall.
----------------------- Details ------------------------
(above:0.2347) (across:0.0556) (against:0.0789) (along:0.2821) (and:0.0968) (at:0.3721) (attached to:0.0288) (behind:0.1802) (belonging to:0.0690) (between:0.0000) (carrying:0.4474) (covered in:0.2857) (covering:0.1286) (eating:0.4762) (flying in:0.0000) (for:0.0741) (from:0.0000) (growing on:0.0000) (hanging from:0.2684) (has:0.4366) (holding:0.3585) (in:0.3171) (in front of:0.2030) (laying on:0.0952) (looking at:0.0435) (lying on:0.1111) (made of:0.0000) (mounted on:0.0000) (near:0.2407) (of:0.3814) (on:0.4620) (on back of:0.0909) (over:0.0976) (painted on:0.0000) (parked on:0.5682) (part of:0.0556) (playing:0.0000) (riding:0.5164) (says:0.0000) (sitting on:0.2882) (standing on:0.1623) (to:0.2778) (under:0.1301) (using:0.2308) (walking in:0.0000) (walking on:0.3405) (watching:0.0588) (wearing:0.6272) (wears:0.3452) (with:0.1711) 
--------------------------------------------------------
====================================================================================================

2022-04-26 12:35:00,951 maskrcnn_benchmark INFO: Validation Result: 0.3554
2022-04-26 12:47:23,447 maskrcnn_benchmark INFO: eta: 9:49:18  iter: 12200  loss: 0.6003 (0.7902)  loss_refine_obj: 0.4822 (0.5349)  loss_rel: 0.1549 (0.2553)  time: 3.7192 (6.0963)  data: 0.3318 (2.2921)  lr: 0.001600  max mem: 10525
2022-04-26 12:59:49,202 maskrcnn_benchmark INFO: eta: 9:25:25  iter: 12400  loss: 0.5937 (0.7872)  loss_refine_obj: 0.4364 (0.5337)  loss_rel: 0.1500 (0.2535)  time: 3.6987 (6.0581)  data: 0.3510 (2.2610)  lr: 0.001600  max mem: 10525
2022-04-26 13:12:33,191 maskrcnn_benchmark INFO: eta: 9:02:02  iter: 12600  loss: 0.6140 (0.7845)  loss_refine_obj: 0.4533 (0.5326)  loss_rel: 0.1370 (0.2518)  time: 3.8775 (6.0226)  data: 0.3202 (2.2306)  lr: 0.001600  max mem: 10525
2022-04-26 13:24:55,568 maskrcnn_benchmark INFO: eta: 8:38:49  iter: 12800  loss: 0.5751 (0.7814)  loss_refine_obj: 0.4266 (0.5313)  loss_rel: 0.1288 (0.2501)  time: 3.5953 (5.9865)  data: 0.3287 (2.2013)  lr: 0.001600  max mem: 10525
2022-04-26 13:37:18,674 maskrcnn_benchmark INFO: eta: 8:15:57  iter: 13000  loss: 0.5974 (0.7787)  loss_refine_obj: 0.4665 (0.5301)  loss_rel: 0.1245 (0.2486)  time: 3.7362 (5.9516)  data: 0.3576 (2.1730)  lr: 0.001600  max mem: 10525
2022-04-26 13:49:25,539 maskrcnn_benchmark INFO: eta: 7:53:18  iter: 13200  loss: 0.6240 (0.7762)  loss_refine_obj: 0.4644 (0.5291)  loss_rel: 0.1477 (0.2471)  time: 3.5963 (5.9164)  data: 0.3394 (2.1454)  lr: 0.001600  max mem: 10525
2022-04-26 14:03:26,632 maskrcnn_benchmark INFO: eta: 7:31:38  iter: 13400  loss: 0.6233 (0.7736)  loss_refine_obj: 0.4785 (0.5280)  loss_rel: 0.1504 (0.2456)  time: 4.6630 (5.8909)  data: 0.3516 (2.1195)  lr: 0.001600  max mem: 10525
2022-04-26 14:19:28,899 maskrcnn_benchmark INFO: eta: 7:10:50  iter: 13600  loss: 0.6050 (0.7712)  loss_refine_obj: 0.4427 (0.5270)  loss_rel: 0.1454 (0.2442)  time: 4.6121 (5.8750)  data: 0.3681 (2.0947)  lr: 0.001600  max mem: 10525
2022-04-26 14:35:40,923 maskrcnn_benchmark INFO: eta: 6:50:13  iter: 13800  loss: 0.5743 (0.7687)  loss_refine_obj: 0.4111 (0.5259)  loss_rel: 0.1513 (0.2428)  time: 4.7693 (5.8603)  data: 0.3651 (2.0710)  lr: 0.001600  max mem: 10525
2022-04-26 14:52:18,180 maskrcnn_benchmark INFO: eta: 6:29:51  iter: 14000  loss: 0.6043 (0.7663)  loss_refine_obj: 0.4482 (0.5249)  loss_rel: 0.1379 (0.2414)  time: 4.8336 (5.8478)  data: 0.3708 (2.0487)  lr: 0.001600  max mem: 10525
2022-04-26 14:52:18,180 maskrcnn_benchmark INFO: Start validating
2022-04-26 14:52:18,225 maskrcnn_benchmark INFO: Start evaluation on VG_stanford_filtered_with_attribute_val dataset(5000 images).
